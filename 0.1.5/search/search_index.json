{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLflow Assistant","text":"<p>A Python package that simplifies working with MLflow by providing streamlined tools and utilities for machine learning experiment tracking and model management.</p>"},{"location":"#what-is-mlflow-assistant","title":"What is MLflow Assistant?","text":"<p>MLflow Assistant enhances your MLflow experience by offering:</p> <ul> <li>Simplified API: Easy-to-use interfaces for common MLflow operations</li> <li>Enhanced Workflows: Streamlined processes for experiment tracking</li> <li>Utility Functions: Helper tools for model management and deployment</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install MLflow Assistant:</p> <pre><code>pip install mlflow-assistant\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation - Setup and installation instructions</li> <li>Code Reference - Complete API documentation</li> <li>Release Notes - What's new in each version</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>MLflow Documentation</li> </ul>"},{"location":"changelog/","title":"CHANGELOG","text":"<p>All notable changes to the <code>mlflow-assistant</code> package will be documented in this file.</p> <p>This project adheres to Semantic Versioning and follows changelog conventions inspired by Keep a Changelog.</p>"},{"location":"changelog/#v015-2025-09-18","title":"[v0.1.5] \u2013 2025-09-18","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Providers This allows users to interact with different LLM services (OpenAI, Ollama and Databricks).</li> <li>Agent Workflow Replaced the mocked answer for an answer generated by a LangGraph agent that connects to MLflow Server via tools.</li> </ul>"},{"location":"changelog/#v014-2025-07-15","title":"[v0.1.4] \u2013 2025-07-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Setup This allows users to setup mlflow-assistant.</li> <li>Start This allows users to start the session to interact with mlflow interactively.</li> </ul>"},{"location":"changelog/#notes","title":"Notes","text":"<ul> <li>Start functionality is currently limited as it is calling a mock function</li> </ul>"},{"location":"changelog/#v013-2025-07-15","title":"[v0.1.3] \u2013 2025-07-15","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed navigation structure in <code>mkdocs.yml</code> to correctly point to <code>reference/</code> directory instead of <code>reference/SUMMARY.md</code></li> <li>Improved compatibility with <code>literate-nav</code> and <code>section-index</code> plugins for proper code reference navigation</li> </ul>"},{"location":"changelog/#v012-2025-07-15","title":"[v0.1.2] \u2013 2025-07-15","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Comprehensive documentation system with MkDocs Material theme</li> <li>Versioned documentation deployment using mike for GitHub Pages</li> <li>Automated documentation generation from code docstrings using mkdocstrings</li> <li>Code reference navigation with auto-generated API documentation</li> <li>Custom Geist font for modern, clean documentation appearance</li> <li>Integrated documentation deployment in the main CI/CD pipeline</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Enhanced CI/CD pipeline to include documentation deployment after successful releases</li> <li>Documentation now automatically deploys on version bumps and PyPI releases</li> </ul>"},{"location":"changelog/#notes_1","title":"Notes","text":"<ul> <li>Documentation is available at https://hugodscarvalho.github.io/mlflow-assistant/</li> <li>Each release gets its own versioned documentation with <code>latest</code> alias pointing to the newest version</li> </ul>"},{"location":"changelog/#v011-2025-07-14","title":"[v0.1.1] \u2013 2025-07-14","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Introduced a unified CI/CD pipeline for:</li> <li>Linting with <code>ruff</code>.</li> <li>Type checking with <code>mypy</code>.</li> <li>Test execution with <code>pytest</code> and coverage reporting via <code>pytest-cov</code>.</li> <li>Publishing to PyPI on new Git tags.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improved test fixture logic for integration tests to correctly check for a live MLflow Tracking Server before execution.</li> </ul>"},{"location":"changelog/#notes_2","title":"Notes","text":"<ul> <li>This pipeline simplifies maintenance by consolidating quality checks and publishing into a single workflow, triggered on pushes and releases.</li> </ul>"},{"location":"changelog/#v010-2025-05-08","title":"[v0.1.0] \u2013 2025-05-08","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Initial placeholder release of <code>mlflow-assistant</code> on PyPI.</li> <li>Included a minimal <code>MLflowClient</code> wrapper module.</li> </ul>"},{"location":"changelog/#notes_3","title":"Notes","text":"<ul> <li>This version was intentionally published with limited functionality to reserve the <code>mlflow-assistant</code> package name on PyPI.</li> <li>Full development will begin in upcoming versions with a focus on making MLflow easier to use through high-level utilities and assistant-like automation features.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides instructions on how to install and get started with <code>mlflow-assistant</code>.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Python: <code>mlflow-assistant</code> requires Python 3.9 or higher. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>or</p> <pre><code>python3 --version\n</code></pre> </li> <li> <p>pip: Python's package installer, <code>pip</code>, is necessary to install <code>mlflow-assistant</code> and its dependencies. It usually comes bundled with Python. You can check if you have it by running:</p> <pre><code>pip --version\n</code></pre> <p>or</p> <pre><code>pip3 --version\n</code></pre> </li> </ul>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":"<p>You can install <code>mlflow-assistant</code> using pip:</p> <pre><code>pip install mlflow-assistant\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mlflow_assistant<ul> <li>cli<ul> <li>commands</li> <li>setup</li> <li>validation</li> </ul> </li> <li>core<ul> <li>cli</li> <li>connection</li> <li>core</li> <li>provider</li> <li>workflow</li> </ul> </li> <li>engine<ul> <li>definitions</li> <li>processor</li> <li>tools</li> <li>workflow</li> </ul> </li> <li>main</li> <li>providers<ul> <li>base</li> <li>databricks_provider</li> <li>definitions</li> <li>ollama_provider</li> <li>openai_provider</li> <li>utilities</li> </ul> </li> <li>utils<ul> <li>config</li> <li>constants</li> <li>definitions</li> <li>exceptions</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlflow_assistant/__init__/","title":"mlflow_assistant","text":""},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant","title":"<code>mlflow_assistant</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli","title":"<code>cli</code>","text":"<p>CLI modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands","title":"<code>commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        asyncio.run(_process_user_query(query, provider_config, verbose))\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.setup","title":"<code>setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    elif current_provider_type == Provider.DATABRICKS.value:\n        config_path = Path(DEFAULT_DATABRICKS_CONFIG_FILE).expanduser()\n        # Verify Databricks configuration file path\n        click.echo(f\"Checking Databricks configuration file at: {config_path}\")\n        if not os.path.isfile(config_path):\n            # File does not exist, prompt user to create it\n            click.echo(\n                    \"Setup aborted. Please setup Databricks config file and try again.\",\n                )\n            return\n\n        # Get Databricks configuration file\n        config_string = Path(config_path).read_text()\n\n        # Get profiles from the Databricks configuration file\n        # Parse the config string\n        databricks_config = configparser.ConfigParser()\n        databricks_config.read_string(config_string)\n\n        # Manually include DEFAULT section\n        all_sections = ['DEFAULT', *databricks_config.sections()]\n\n        profile_options = [section for section in all_sections if 'token' in databricks_config[section]]\n\n        if not profile_options:\n            click.echo(\n                \"\\n\u26a0\ufe0f  No valid profiles found in Databricks configuration file.\",\n            )\n            click.echo(\n                \"Please ensure your Databricks config file contains a profile with a 'token'.\",\n            )\n            click.echo(\n                \"Setup aborted. Please fix the configuration and try again.\",\n            )\n            return\n\n        profile = click.prompt(\n            \"\\nWhich databricks profile would you like to use?\",\n            type=click.Choice(profile_options, case_sensitive=False),\n            default=profile_options[0],\n        )\n\n        # Peompt for model name\n        databricks_model = click.prompt(\n            \"Enter the Databricks model to use\",\n        )\n\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: profile,\n            CONFIG_KEY_MODEL: databricks_model,\n        }\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation","title":"<code>validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core","title":"<code>core</code>","text":"<p>Core functionality for MLflow Assistant.</p> <p>This subpackage contains the core modules for managing connections, workflows, and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.cli","title":"<code>cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection","title":"<code>connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.debug(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.debug(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core","title":"<code>core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.provider","title":"<code>provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.workflow","title":"<code>workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine","title":"<code>engine</code>","text":"<p>MLflow Assistant Engine - Provides workflow functionality to process user query.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.definitions","title":"<code>definitions</code>","text":"<p>Constants for the MLflow Assistant engine.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.processor","title":"<code>processor</code>","text":"<p>Query processor that leverages the workflow engine for processing user queries and generating responses using an AI provider.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.processor.process_query","title":"<code>process_query(query, provider_config, verbose=False)</code>  <code>async</code>","text":"<p>Process a query through the MLflow Assistant workflow.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to process</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing the response</p> Source code in <code>src/mlflow_assistant/engine/processor.py</code> <pre><code>async def process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Process a query through the MLflow Assistant workflow.\n\n    Args:\n        query: The query to process\n        provider_config: AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dict containing the response\n\n    \"\"\"\n    import time\n\n    from .workflow import create_workflow\n\n    # Track start time for duration calculation\n    start_time = time.time()\n\n    try:\n        # Create workflow\n        workflow = create_workflow()\n\n        # Run workflow with provider config\n        initial_state = {\n            STATE_KEY_MESSAGES: [HumanMessage(content=query)],\n            STATE_KEY_PROVIDER_CONFIG: provider_config,\n        }\n\n        if verbose:\n            logger.info(f\"Running workflow with query: {query}\")\n            logger.info(f\"Using provider: {provider_config.get(CONFIG_KEY_TYPE)}\")\n            logger.info(\n                f\"Using model: {provider_config.get(CONFIG_KEY_MODEL, 'default')}\",\n            )\n\n        result = await workflow.ainvoke(initial_state)\n\n        # Calculate duration\n        duration = time.time() - start_time\n\n        return {\n            \"original_query\": query,\n            \"response\": result.get(STATE_KEY_MESSAGES)[-1],\n            \"duration\": duration,  # Add duration to response\n        }\n\n    except Exception as e:\n        # Calculate duration even for errors\n        duration = time.time() - start_time\n\n        logger.error(f\"Error processing query: {e}\")\n\n        return {\n            \"error\": str(e),\n            \"original_query\": query,\n            \"response\": f\"Error processing query: {e!s}\",\n        }\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools","title":"<code>tools</code>","text":"<p>LangGraph tools for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.MLflowTools","title":"<code>MLflowTools</code>","text":"<p>Collection of helper utilities for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.MLflowTools.format_timestamp","title":"<code>format_timestamp(timestamp_ms)</code>  <code>staticmethod</code>","text":"<p>Convert a millisecond timestamp to a human-readable string.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@staticmethod\ndef format_timestamp(timestamp_ms: int) -&gt; str:\n    \"\"\"Convert a millisecond timestamp to a human-readable string.\"\"\"\n    if not timestamp_ms:\n        return NA\n    dt = datetime.fromtimestamp(timestamp_ms / 1000.0)\n    return dt.strftime(TIME_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.get_model_details","title":"<code>get_model_details(model_name)</code>","text":"<p>Get detailed information about a specific registered model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the registered model</p> required <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing detailed information about the model.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_model_details(model_name: str) -&gt; str:\n    \"\"\"Get detailed information about a specific registered model.\n\n    Args:\n        model_name: The name of the registered model\n\n    Returns:\n        A JSON string containing detailed information about the model.\n\n    \"\"\"\n    logger.debug(f\"Fetching details for model: {model_name}\")\n\n    try:\n        # Get the registered model\n        model = client.get_registered_model(model_name)\n\n        model_info = {\n            \"name\": model.name,\n            \"creation_timestamp\": MLflowTools.format_timestamp(\n                model.creation_timestamp,\n            ),\n            \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                model.last_updated_timestamp,\n            ),\n            \"description\": model.description or \"\",\n            \"tags\": {tag.key: tag.value for tag in model.tags}\n            if hasattr(model, \"tags\")\n            else {},\n            \"versions\": [],\n        }\n\n        # Get all versions for this model\n        versions = client.search_model_versions(f\"name='{model_name}'\")\n\n        for version in versions:\n            version_info = {\n                \"version\": version.version,\n                \"status\": version.status,\n                \"stage\": version.current_stage,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    version.creation_timestamp,\n                ),\n                \"source\": version.source,\n                \"run_id\": version.run_id,\n            }\n\n            # Get additional information about the run if available\n            if version.run_id:\n                try:\n                    run = client.get_run(version.run_id)\n                    # Extract only essential run information to avoid serialization issues\n                    run_metrics = {}\n                    for k, v in run.data.metrics.items():\n                        try:\n                            run_metrics[k] = float(v)\n                        except ValueError:\n                            run_metrics[k] = str(v)\n\n                    version_info[\"run\"] = {\n                        \"status\": run.info.status,\n                        \"start_time\": MLflowTools.format_timestamp(\n                            run.info.start_time,\n                        ),\n                        \"end_time\": MLflowTools.format_timestamp(run.info.end_time)\n                        if run.info.end_time\n                        else None,\n                        \"metrics\": run_metrics,\n                    }\n                except Exception as e:\n                    logger.warning(\n                        f\"Error getting run details for {version.run_id}: {e!s}\",\n                    )\n                    version_info[\"run\"] = \"Error retrieving run details\"\n\n            model_info[\"versions\"].append(version_info)\n\n        return json.dumps(model_info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting model details: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Get information about the MLflow tracking server and system.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing system information.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_system_info() -&gt; str:\n    \"\"\"Get information about the MLflow tracking server and system.\n\n    Returns:\n        A JSON string containing system information.\n\n    \"\"\"\n    logger.debug(\"Getting MLflow system information\")\n\n    try:\n        info = {\n            \"mlflow_version\": mlflow.__version__,\n            \"tracking_uri\": mlflow.get_tracking_uri(),\n            \"registry_uri\": mlflow.get_registry_uri(),\n            \"artifact_uri\": mlflow.get_artifact_uri(),\n            \"python_version\": sys.version,\n            \"server_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n\n        # Get experiment count\n        try:\n            experiments = client.search_experiments()\n            info[\"experiment_count\"] = len(experiments)\n        except Exception as e:\n            logger.warning(f\"Error getting experiment count: {e!s}\")\n            info[\"experiment_count\"] = \"Error retrieving count\"\n\n        # Get model count\n        try:\n            models = client.search_registered_models()\n            info[\"model_count\"] = len(models)\n        except Exception as e:\n            logger.warning(f\"Error getting model count: {e!s}\")\n            info[\"model_count\"] = \"Error retrieving count\"\n\n        # Get active run count\n        try:\n            active_runs = 0\n            for exp in experiments:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id],\n                    filter_string=\"attributes.status = 'RUNNING'\",\n                    max_results=1000,\n                )\n                active_runs += len(runs)\n\n            info[\"active_runs\"] = active_runs\n        except Exception as e:\n            logger.warning(f\"Error getting active run count: {e!s}\")\n            info[\"active_runs\"] = \"Error retrieving count\"\n\n        return json.dumps(info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting system info: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.list_experiments","title":"<code>list_experiments(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all experiments in the MLflow tracking server, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include experiments whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all experiments matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_experiments(\n    name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS,\n) -&gt; str:\n    \"\"\"List all experiments in the MLflow tracking server, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include experiments whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all experiments matching the criteria.\n\n    \"\"\"\n    logger.debug(f\"Fetching experiments (filter: '{name_contains}', max: {max_results})\")\n\n    try:\n        # Get all experiments\n        experiments = client.search_experiments()\n\n        # Filter by name if specified\n        if name_contains:\n            experiments = [\n                exp for exp in experiments if name_contains.lower() in exp.name.lower()\n            ]\n\n        # Limit to max_results\n        experiments = experiments[:max_results]\n\n        # Create a list to hold experiment information\n        experiments_info = []\n\n        # Extract relevant information for each experiment\n        for exp in experiments:\n            exp_info = {\n                \"experiment_id\": exp.experiment_id,\n                \"name\": exp.name,\n                \"artifact_location\": exp.artifact_location,\n                \"lifecycle_stage\": exp.lifecycle_stage,\n                \"creation_time\": MLflowTools.format_timestamp(exp.creation_time)\n                if hasattr(exp, \"creation_time\")\n                else None,\n                \"tags\": {tag.key: tag.value for tag in exp.tags}\n                if hasattr(exp, \"tags\")\n                else {},\n            }\n\n            # Get the run count for this experiment\n            try:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id], max_results=1,\n                )\n                if runs:\n                    # Just get the count of runs, not the actual runs\n                    run_count = client.search_runs(\n                        experiment_ids=[exp.experiment_id], max_results=1000,\n                    )\n                    exp_info[\"run_count\"] = len(run_count)\n                else:\n                    exp_info[\"run_count\"] = 0\n            except Exception as e:\n                logger.warning(\n                    f\"Error getting run count for experiment {exp.experiment_id}: {e!s}\",\n                )\n                exp_info[\"run_count\"] = \"Error getting count\"\n\n            experiments_info.append(exp_info)\n\n        result = {\n            \"total_experiments\": len(experiments_info),\n            \"experiments\": experiments_info,\n        }\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing experiments: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.tools.list_models","title":"<code>list_models(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all registered models in the MLflow model registry, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include models whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all registered models matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_models(name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS) -&gt; str:\n    \"\"\"List all registered models in the MLflow model registry, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include models whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all registered models matching the criteria.\n\n    \"\"\"\n    logger.debug(\n        f\"Fetching registered models (filter: '{name_contains}', max: {max_results})\",\n    )\n\n    try:\n        # Get all registered models\n        registered_models = client.search_registered_models(max_results=max_results)\n\n        # Filter by name if specified\n        if name_contains:\n            registered_models = [\n                model\n                for model in registered_models\n                if name_contains.lower() in model.name.lower()\n            ]\n\n        # Create a list to hold model information\n        models_info = []\n\n        # Extract relevant information for each model\n        for model in registered_models:\n            model_info = {\n                \"name\": model.name,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    model.creation_timestamp,\n                ),\n                \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                    model.last_updated_timestamp,\n                ),\n                \"description\": model.description or \"\",\n                \"tags\": {tag.key: tag.value for tag in model.tags}\n                if hasattr(model, \"tags\")\n                else {},\n                \"latest_versions\": [],\n            }\n\n            # Add the latest versions if available\n            if model.latest_versions and len(model.latest_versions) &gt; 0:\n                for version in model.latest_versions:\n                    version_info = {\n                        \"version\": version.version,\n                        \"status\": version.status,\n                        \"stage\": version.current_stage,\n                        \"creation_timestamp\": MLflowTools.format_timestamp(\n                            version.creation_timestamp,\n                        ),\n                        \"run_id\": version.run_id,\n                    }\n                    model_info[\"latest_versions\"].append(version_info)\n\n            models_info.append(model_info)\n\n        result = {\"total_models\": len(models_info), \"models\": models_info}\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing models: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.workflow","title":"<code>workflow</code>","text":"<p>Core LangGraph-based workflow engine for processing user queries and generating responses using an AI provider.</p> <p>This workflow supports tool-augmented generation: tool calls are detected and executed in a loop until a final AI response is produced.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.workflow.State","title":"<code>State</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>State schema for the workflow engine.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.engine.workflow.create_workflow","title":"<code>create_workflow()</code>","text":"<p>Create and return a compiled LangGraph workflow.</p> Source code in <code>src/mlflow_assistant/engine/workflow.py</code> <pre><code>def create_workflow():\n    \"\"\"Create and return a compiled LangGraph workflow.\"\"\"\n    graph_builder = StateGraph(State)\n\n    def call_model(state: State) -&gt; State:\n        \"\"\"Call the AI model and return updated state with response.\"\"\"\n        messages = state[STATE_KEY_MESSAGES]\n        provider_config = state.get(STATE_KEY_PROVIDER_CONFIG, {})\n        try:\n            provider = AIProvider.create(provider_config)\n            model = provider.langchain_model().bind_tools(tools)\n            response = model.invoke(messages)\n            return {**state, STATE_KEY_MESSAGES: [response]}\n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\", exc_info=True)\n            return {**state, STATE_KEY_MESSAGES: messages}\n\n    # Add nodes\n    graph_builder.add_node(\"tools\", ToolNode(tools))\n    graph_builder.add_node(\"model\", call_model)\n\n    # Define graph transitions\n    graph_builder.add_edge(\"tools\", \"model\")\n    graph_builder.add_conditional_edges(\"model\", tools_condition)\n    graph_builder.set_entry_point(\"model\")\n\n    return graph_builder.compile()\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.main","title":"<code>main</code>","text":"<p>Main entry point for executing the MLflow Assistant package directly.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers","title":"<code>providers</code>","text":"<p>Provider module for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.AIProvider","title":"<code>AIProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.AIProvider.langchain_model","title":"<code>langchain_model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the underlying LangChain model.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.AIProvider.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Auto-register provider subclasses.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Auto-register provider subclasses.\"\"\"\n    super().__init_subclass__(**kwargs)\n    # Register the provider using the class name\n    provider_type = cls.__name__.lower().replace(CONFIG_KEY_PROVIDER, \"\")\n    AIProvider._providers[provider_type] = cls\n    logger.debug(f\"Registered provider: {provider_type}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.AIProvider.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an AI provider based on configuration.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>@classmethod\ndef create(cls, config: dict[str, Any]) -&gt; \"AIProvider\":\n    \"\"\"Create an AI provider based on configuration.\"\"\"\n    provider_type = config.get(CONFIG_KEY_TYPE)\n\n    if not provider_type:\n        error_msg = \"Provider type not specified in configuration\"\n        raise ValueError(error_msg)\n\n    provider_type = provider_type.lower()\n\n    # Extract common parameters\n    kwargs = {}\n    for param in ParameterKeys.PARAMETERS_ALL:\n        if param in config:\n            kwargs[param] = config[param]\n\n    # Import providers dynamically to avoid circular imports\n    if provider_type == Provider.OPENAI.value:\n        from .openai_provider import OpenAIProvider\n\n        logger.debug(\n            f\"Creating OpenAI provider with model {config.get(CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI))}\",\n        )\n        return OpenAIProvider(\n            api_key=config.get(CONFIG_KEY_API_KEY),\n            model=config.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OPENAI),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.OLLAMA.value:\n        from .ollama_provider import OllamaProvider\n\n        logger.debug(\n            f\"Creating Ollama provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return OllamaProvider(\n            uri=config.get(CONFIG_KEY_URI),\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OLLAMA),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.DATABRICKS.value:\n        from .databricks_provider import DatabricksProvider\n\n        logger.debug(\n            f\"Creating Databricks provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return DatabricksProvider(\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.DATABRICKS),\n            ),\n            **kwargs,\n        )\n    if provider_type not in cls._providers:\n        error_msg = f\"Unknown provider type: {provider_type}. Available types: {', '.join(cls._providers.keys())}\"\n        raise ValueError(error_msg)\n    # Generic initialization for future providers\n    provider_class = cls._providers[provider_type]\n    return provider_class(config)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.get_ollama_models","title":"<code>get_ollama_models(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Fetch the list of available Ollama models.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def get_ollama_models(uri: str = DEFAULT_OLLAMA_URI) -&gt; list:\n    \"\"\"Fetch the list of available Ollama models.\"\"\"\n    # Try using direct API call first\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=10)\n        if response.status_code == 200:\n            data = response.json()\n            models = [m.get(\"name\") for m in data.get(\"models\", [])]\n            if models:\n                return models\n    except Exception as e:\n        logger.debug(f\"Failed to get Ollama models from API: {e}\")\n\n    try:\n        # Execute the Ollama list command\n        ollama_path = shutil.which(\"ollama\")\n\n        result = subprocess.run(  # noqa: S603\n            [ollama_path, \"list\"], capture_output=True, text=True, check=False,\n        )\n\n        # Check if command executed successfully\n        if result.returncode != 0:\n            logger.warning(f\"ollama list failed: {result.stderr}\")\n            return FALLBACK_MODELS\n\n        # Parse the output to extract model names\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) &lt;= 1:  # Only header line or empty\n            return FALLBACK_MODELS\n\n        # Skip header line and extract the first column (model name)\n        models = [line.split()[0] for line in lines[1:]]\n        return models or FALLBACK_MODELS\n\n    except (subprocess.SubprocessError, FileNotFoundError, IndexError) as e:\n        logger.warning(f\"Error fetching Ollama models: {e!s}\")\n        return FALLBACK_MODELS\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.verify_ollama_running","title":"<code>verify_ollama_running(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Verify if Ollama is running at the given URI.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def verify_ollama_running(uri: str = DEFAULT_OLLAMA_URI) -&gt; bool:\n    \"\"\"Verify if Ollama is running at the given URI.\"\"\"\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=2)\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.base","title":"<code>base</code>","text":"<p>Base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.base.AIProvider","title":"<code>AIProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.base.AIProvider.langchain_model","title":"<code>langchain_model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the underlying LangChain model.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.base.AIProvider.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Auto-register provider subclasses.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Auto-register provider subclasses.\"\"\"\n    super().__init_subclass__(**kwargs)\n    # Register the provider using the class name\n    provider_type = cls.__name__.lower().replace(CONFIG_KEY_PROVIDER, \"\")\n    AIProvider._providers[provider_type] = cls\n    logger.debug(f\"Registered provider: {provider_type}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.base.AIProvider.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an AI provider based on configuration.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>@classmethod\ndef create(cls, config: dict[str, Any]) -&gt; \"AIProvider\":\n    \"\"\"Create an AI provider based on configuration.\"\"\"\n    provider_type = config.get(CONFIG_KEY_TYPE)\n\n    if not provider_type:\n        error_msg = \"Provider type not specified in configuration\"\n        raise ValueError(error_msg)\n\n    provider_type = provider_type.lower()\n\n    # Extract common parameters\n    kwargs = {}\n    for param in ParameterKeys.PARAMETERS_ALL:\n        if param in config:\n            kwargs[param] = config[param]\n\n    # Import providers dynamically to avoid circular imports\n    if provider_type == Provider.OPENAI.value:\n        from .openai_provider import OpenAIProvider\n\n        logger.debug(\n            f\"Creating OpenAI provider with model {config.get(CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI))}\",\n        )\n        return OpenAIProvider(\n            api_key=config.get(CONFIG_KEY_API_KEY),\n            model=config.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OPENAI),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.OLLAMA.value:\n        from .ollama_provider import OllamaProvider\n\n        logger.debug(\n            f\"Creating Ollama provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return OllamaProvider(\n            uri=config.get(CONFIG_KEY_URI),\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OLLAMA),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.DATABRICKS.value:\n        from .databricks_provider import DatabricksProvider\n\n        logger.debug(\n            f\"Creating Databricks provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return DatabricksProvider(\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.DATABRICKS),\n            ),\n            **kwargs,\n        )\n    if provider_type not in cls._providers:\n        error_msg = f\"Unknown provider type: {provider_type}. Available types: {', '.join(cls._providers.keys())}\"\n        raise ValueError(error_msg)\n    # Generic initialization for future providers\n    provider_class = cls._providers[provider_type]\n    return provider_class(config)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.databricks_provider","title":"<code>databricks_provider</code>","text":"<p>Databricks provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.databricks_provider.DatabricksProvider","title":"<code>DatabricksProvider(model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Databricks provider implementation.</p> <p>Initialize the Databricks provider with model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def __init__(\n    self,\n    model: str | None = None,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the Databricks provider with model.\"\"\"\n    self.model_name = (\n        model or Provider.get_default_model(Provider.DATABRICKS.value)\n    )\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.DATABRICKS.value)\n    )\n    self.kwargs = kwargs\n\n    for var in DATABRICKS_CREDENTIALS:\n        if var not in os.environ:\n            logger.warning(\n                f\"Missing environment variable: {var}. \"\n                \"Responses may fail if you are running outside Databricks.\",\n            )\n\n    # Build parameters dict with only non-None values\n    model_params = {\"endpoint\": self.model_name, \"temperature\": temperature}\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.DATABRICKS.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatDatabricks(**model_params)\n\n    logger.debug(f\"Databricks provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.databricks_provider.DatabricksProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.definitions","title":"<code>definitions</code>","text":"<p>Constants for the MLflow Assistant providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.definitions.ParameterKeys","title":"<code>ParameterKeys</code>","text":"<p>Keys and default parameter groupings for supported providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.definitions.ParameterKeys.get_parameters","title":"<code>get_parameters(provider)</code>  <code>classmethod</code>","text":"<p>Return the list of parameters for the given provider name.</p> Source code in <code>src/mlflow_assistant/providers/definitions.py</code> <pre><code>@classmethod\ndef get_parameters(cls, provider: str) -&gt; list[str]:\n    \"\"\"Return the list of parameters for the given provider name.\"\"\"\n    provider_map = {\n        \"openai\": cls.PARAMETERS_OPENAI,\n        \"ollama\": cls.PARAMETERS_OLLAMA,\n        \"databricks\": cls.PARAMETERS_DATABRICKS,\n    }\n    return provider_map.get(provider.lower(), [])\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.ollama_provider","title":"<code>ollama_provider</code>","text":"<p>Ollama provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.ollama_provider.OllamaProvider","title":"<code>OllamaProvider(uri=None, model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Ollama provider implementation.</p> <p>Initialize the Ollama provider with URI and model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def __init__(self, uri=None, model=None, temperature=None, **kwargs):\n    \"\"\"Initialize the Ollama provider with URI and model.\"\"\"\n    # Handle None URI case to prevent attribute errors\n    if uri is None:\n        logger.warning(\n            f\"Ollama URI is None. Using default URI: {DEFAULT_OLLAMA_URI}\",\n        )\n        self.uri = DEFAULT_OLLAMA_URI\n    else:\n        self.uri = uri.rstrip(\"/\")\n\n    self.model_name = model or OllamaModel.LLAMA32.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OLLAMA.value)\n    )\n\n    # Store kwargs for later use when creating specialized models\n    self.kwargs = kwargs\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"base_url\": self.uri,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Use langchain-ollama's dedicated ChatOllama class\n    self.model = ChatOllama(**model_params)\n\n    logger.debug(\n        f\"Ollama provider initialized with model {self.model_name} at {self.uri}\",\n    )\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.ollama_provider.OllamaProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.openai_provider","title":"<code>openai_provider</code>","text":"<p>OpenAI provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.openai_provider.OpenAIProvider","title":"<code>OpenAIProvider(api_key=None, model=OpenAIModel.GPT35.value, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>OpenAI provider implementation.</p> <p>Initialize the OpenAI provider with API key and model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def __init__(\n    self,\n    api_key=None,\n    model=OpenAIModel.GPT35.value,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the OpenAI provider with API key and model.\"\"\"\n    self.api_key = api_key\n    self.model_name = model or OpenAIModel.GPT35.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OPENAI.value)\n    )\n    self.kwargs = kwargs\n\n    if not self.api_key:\n        logger.warning(\"No OpenAI API key provided. Responses may fail.\")\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"api_key\": api_key,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatOpenAI(**model_params)\n\n    logger.debug(f\"OpenAI provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.openai_provider.OpenAIProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.utilities","title":"<code>utilities</code>","text":"<p>Providers utilities.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.utilities.get_ollama_models","title":"<code>get_ollama_models(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Fetch the list of available Ollama models.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def get_ollama_models(uri: str = DEFAULT_OLLAMA_URI) -&gt; list:\n    \"\"\"Fetch the list of available Ollama models.\"\"\"\n    # Try using direct API call first\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=10)\n        if response.status_code == 200:\n            data = response.json()\n            models = [m.get(\"name\") for m in data.get(\"models\", [])]\n            if models:\n                return models\n    except Exception as e:\n        logger.debug(f\"Failed to get Ollama models from API: {e}\")\n\n    try:\n        # Execute the Ollama list command\n        ollama_path = shutil.which(\"ollama\")\n\n        result = subprocess.run(  # noqa: S603\n            [ollama_path, \"list\"], capture_output=True, text=True, check=False,\n        )\n\n        # Check if command executed successfully\n        if result.returncode != 0:\n            logger.warning(f\"ollama list failed: {result.stderr}\")\n            return FALLBACK_MODELS\n\n        # Parse the output to extract model names\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) &lt;= 1:  # Only header line or empty\n            return FALLBACK_MODELS\n\n        # Skip header line and extract the first column (model name)\n        models = [line.split()[0] for line in lines[1:]]\n        return models or FALLBACK_MODELS\n\n    except (subprocess.SubprocessError, FileNotFoundError, IndexError) as e:\n        logger.warning(f\"Error fetching Ollama models: {e!s}\")\n        return FALLBACK_MODELS\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.providers.utilities.verify_ollama_running","title":"<code>verify_ollama_running(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Verify if Ollama is running at the given URI.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def verify_ollama_running(uri: str = DEFAULT_OLLAMA_URI) -&gt; bool:\n    \"\"\"Verify if Ollama is running at the given URI.\"\"\"\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=2)\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils","title":"<code>utils</code>","text":"<p>Utility modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n        cls.DATABRICKS: DatabricksModel.DATABRICKS_META_LLAMA3.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Provider.get_default_temperature","title":"<code>get_default_temperature(provider)</code>  <code>classmethod</code>","text":"<p>Get the default temperature for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_temperature(cls, provider):\n    \"\"\"Get the default temperature for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: 0.7,\n        cls.DATABRICKS: 0.7,\n        cls.OLLAMA: 0.7,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    if provider_type == Provider.DATABRICKS.value:\n        # Set environment variables for Databricks profile\n        _set_environment_variables(provider.get(CONFIG_KEY_PROFILE))\n\n        return {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: provider.get(CONFIG_KEY_PROFILE),\n            CONFIG_KEY_MODEL: provider.get(CONFIG_KEY_MODEL),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config","title":"<code>config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    if provider_type == Provider.DATABRICKS.value:\n        # Set environment variables for Databricks profile\n        _set_environment_variables(provider.get(CONFIG_KEY_PROFILE))\n\n        return {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: provider.get(CONFIG_KEY_PROFILE),\n            CONFIG_KEY_MODEL: provider.get(CONFIG_KEY_MODEL),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants","title":"<code>constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.DatabricksModel","title":"<code>DatabricksModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Databricks models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.DatabricksModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Databricks model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Databricks model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n        cls.DATABRICKS: DatabricksModel.DATABRICKS_META_LLAMA3.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Provider.get_default_temperature","title":"<code>get_default_temperature(provider)</code>  <code>classmethod</code>","text":"<p>Get the default temperature for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_temperature(cls, provider):\n    \"\"\"Get the default temperature for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: 0.7,\n        cls.DATABRICKS: 0.7,\n        cls.OLLAMA: 0.7,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions","title":"<code>definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/main/","title":"main","text":""},{"location":"reference/mlflow_assistant/main/#mlflow_assistant.main","title":"<code>mlflow_assistant.main</code>","text":"<p>Main entry point for executing the MLflow Assistant package directly.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/","title":"cli","text":""},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli","title":"<code>mlflow_assistant.cli</code>","text":"<p>CLI modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands","title":"<code>commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        asyncio.run(_process_user_query(query, provider_config, verbose))\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.setup","title":"<code>setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    elif current_provider_type == Provider.DATABRICKS.value:\n        config_path = Path(DEFAULT_DATABRICKS_CONFIG_FILE).expanduser()\n        # Verify Databricks configuration file path\n        click.echo(f\"Checking Databricks configuration file at: {config_path}\")\n        if not os.path.isfile(config_path):\n            # File does not exist, prompt user to create it\n            click.echo(\n                    \"Setup aborted. Please setup Databricks config file and try again.\",\n                )\n            return\n\n        # Get Databricks configuration file\n        config_string = Path(config_path).read_text()\n\n        # Get profiles from the Databricks configuration file\n        # Parse the config string\n        databricks_config = configparser.ConfigParser()\n        databricks_config.read_string(config_string)\n\n        # Manually include DEFAULT section\n        all_sections = ['DEFAULT', *databricks_config.sections()]\n\n        profile_options = [section for section in all_sections if 'token' in databricks_config[section]]\n\n        if not profile_options:\n            click.echo(\n                \"\\n\u26a0\ufe0f  No valid profiles found in Databricks configuration file.\",\n            )\n            click.echo(\n                \"Please ensure your Databricks config file contains a profile with a 'token'.\",\n            )\n            click.echo(\n                \"Setup aborted. Please fix the configuration and try again.\",\n            )\n            return\n\n        profile = click.prompt(\n            \"\\nWhich databricks profile would you like to use?\",\n            type=click.Choice(profile_options, case_sensitive=False),\n            default=profile_options[0],\n        )\n\n        # Peompt for model name\n        databricks_model = click.prompt(\n            \"Enter the Databricks model to use\",\n        )\n\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: profile,\n            CONFIG_KEY_MODEL: databricks_model,\n        }\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation","title":"<code>validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/","title":"commands","text":""},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands","title":"<code>mlflow_assistant.cli.commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        asyncio.run(_process_user_query(query, provider_config, verbose))\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/setup/","title":"setup","text":""},{"location":"reference/mlflow_assistant/cli/setup/#mlflow_assistant.cli.setup","title":"<code>mlflow_assistant.cli.setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/cli/setup/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    elif current_provider_type == Provider.DATABRICKS.value:\n        config_path = Path(DEFAULT_DATABRICKS_CONFIG_FILE).expanduser()\n        # Verify Databricks configuration file path\n        click.echo(f\"Checking Databricks configuration file at: {config_path}\")\n        if not os.path.isfile(config_path):\n            # File does not exist, prompt user to create it\n            click.echo(\n                    \"Setup aborted. Please setup Databricks config file and try again.\",\n                )\n            return\n\n        # Get Databricks configuration file\n        config_string = Path(config_path).read_text()\n\n        # Get profiles from the Databricks configuration file\n        # Parse the config string\n        databricks_config = configparser.ConfigParser()\n        databricks_config.read_string(config_string)\n\n        # Manually include DEFAULT section\n        all_sections = ['DEFAULT', *databricks_config.sections()]\n\n        profile_options = [section for section in all_sections if 'token' in databricks_config[section]]\n\n        if not profile_options:\n            click.echo(\n                \"\\n\u26a0\ufe0f  No valid profiles found in Databricks configuration file.\",\n            )\n            click.echo(\n                \"Please ensure your Databricks config file contains a profile with a 'token'.\",\n            )\n            click.echo(\n                \"Setup aborted. Please fix the configuration and try again.\",\n            )\n            return\n\n        profile = click.prompt(\n            \"\\nWhich databricks profile would you like to use?\",\n            type=click.Choice(profile_options, case_sensitive=False),\n            default=profile_options[0],\n        )\n\n        # Peompt for model name\n        databricks_model = click.prompt(\n            \"Enter the Databricks model to use\",\n        )\n\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: profile,\n            CONFIG_KEY_MODEL: databricks_model,\n        }\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/","title":"validation","text":""},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation","title":"<code>mlflow_assistant.cli.validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/","title":"core","text":""},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core","title":"<code>mlflow_assistant.core</code>","text":"<p>Core functionality for MLflow Assistant.</p> <p>This subpackage contains the core modules for managing connections, workflows, and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.cli","title":"<code>cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection","title":"<code>connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.debug(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.debug(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core","title":"<code>core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.provider","title":"<code>provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.workflow","title":"<code>workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/core/cli/","title":"cli","text":""},{"location":"reference/mlflow_assistant/core/cli/#mlflow_assistant.core.cli","title":"<code>mlflow_assistant.core.cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/core/connection/","title":"connection","text":""},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection","title":"<code>mlflow_assistant.core.connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.debug(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.debug(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/core/core/","title":"core","text":""},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core","title":"<code>mlflow_assistant.core.core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/core/provider/","title":"provider","text":""},{"location":"reference/mlflow_assistant/core/provider/#mlflow_assistant.core.provider","title":"<code>mlflow_assistant.core.provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/core/workflow/","title":"workflow","text":""},{"location":"reference/mlflow_assistant/core/workflow/#mlflow_assistant.core.workflow","title":"<code>mlflow_assistant.core.workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/","title":"engine","text":""},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine","title":"<code>mlflow_assistant.engine</code>","text":"<p>MLflow Assistant Engine - Provides workflow functionality to process user query.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.definitions","title":"<code>definitions</code>","text":"<p>Constants for the MLflow Assistant engine.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.processor","title":"<code>processor</code>","text":"<p>Query processor that leverages the workflow engine for processing user queries and generating responses using an AI provider.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.processor.process_query","title":"<code>process_query(query, provider_config, verbose=False)</code>  <code>async</code>","text":"<p>Process a query through the MLflow Assistant workflow.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to process</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing the response</p> Source code in <code>src/mlflow_assistant/engine/processor.py</code> <pre><code>async def process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Process a query through the MLflow Assistant workflow.\n\n    Args:\n        query: The query to process\n        provider_config: AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dict containing the response\n\n    \"\"\"\n    import time\n\n    from .workflow import create_workflow\n\n    # Track start time for duration calculation\n    start_time = time.time()\n\n    try:\n        # Create workflow\n        workflow = create_workflow()\n\n        # Run workflow with provider config\n        initial_state = {\n            STATE_KEY_MESSAGES: [HumanMessage(content=query)],\n            STATE_KEY_PROVIDER_CONFIG: provider_config,\n        }\n\n        if verbose:\n            logger.info(f\"Running workflow with query: {query}\")\n            logger.info(f\"Using provider: {provider_config.get(CONFIG_KEY_TYPE)}\")\n            logger.info(\n                f\"Using model: {provider_config.get(CONFIG_KEY_MODEL, 'default')}\",\n            )\n\n        result = await workflow.ainvoke(initial_state)\n\n        # Calculate duration\n        duration = time.time() - start_time\n\n        return {\n            \"original_query\": query,\n            \"response\": result.get(STATE_KEY_MESSAGES)[-1],\n            \"duration\": duration,  # Add duration to response\n        }\n\n    except Exception as e:\n        # Calculate duration even for errors\n        duration = time.time() - start_time\n\n        logger.error(f\"Error processing query: {e}\")\n\n        return {\n            \"error\": str(e),\n            \"original_query\": query,\n            \"response\": f\"Error processing query: {e!s}\",\n        }\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools","title":"<code>tools</code>","text":"<p>LangGraph tools for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.MLflowTools","title":"<code>MLflowTools</code>","text":"<p>Collection of helper utilities for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.MLflowTools.format_timestamp","title":"<code>format_timestamp(timestamp_ms)</code>  <code>staticmethod</code>","text":"<p>Convert a millisecond timestamp to a human-readable string.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@staticmethod\ndef format_timestamp(timestamp_ms: int) -&gt; str:\n    \"\"\"Convert a millisecond timestamp to a human-readable string.\"\"\"\n    if not timestamp_ms:\n        return NA\n    dt = datetime.fromtimestamp(timestamp_ms / 1000.0)\n    return dt.strftime(TIME_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.get_model_details","title":"<code>get_model_details(model_name)</code>","text":"<p>Get detailed information about a specific registered model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the registered model</p> required <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing detailed information about the model.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_model_details(model_name: str) -&gt; str:\n    \"\"\"Get detailed information about a specific registered model.\n\n    Args:\n        model_name: The name of the registered model\n\n    Returns:\n        A JSON string containing detailed information about the model.\n\n    \"\"\"\n    logger.debug(f\"Fetching details for model: {model_name}\")\n\n    try:\n        # Get the registered model\n        model = client.get_registered_model(model_name)\n\n        model_info = {\n            \"name\": model.name,\n            \"creation_timestamp\": MLflowTools.format_timestamp(\n                model.creation_timestamp,\n            ),\n            \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                model.last_updated_timestamp,\n            ),\n            \"description\": model.description or \"\",\n            \"tags\": {tag.key: tag.value for tag in model.tags}\n            if hasattr(model, \"tags\")\n            else {},\n            \"versions\": [],\n        }\n\n        # Get all versions for this model\n        versions = client.search_model_versions(f\"name='{model_name}'\")\n\n        for version in versions:\n            version_info = {\n                \"version\": version.version,\n                \"status\": version.status,\n                \"stage\": version.current_stage,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    version.creation_timestamp,\n                ),\n                \"source\": version.source,\n                \"run_id\": version.run_id,\n            }\n\n            # Get additional information about the run if available\n            if version.run_id:\n                try:\n                    run = client.get_run(version.run_id)\n                    # Extract only essential run information to avoid serialization issues\n                    run_metrics = {}\n                    for k, v in run.data.metrics.items():\n                        try:\n                            run_metrics[k] = float(v)\n                        except ValueError:\n                            run_metrics[k] = str(v)\n\n                    version_info[\"run\"] = {\n                        \"status\": run.info.status,\n                        \"start_time\": MLflowTools.format_timestamp(\n                            run.info.start_time,\n                        ),\n                        \"end_time\": MLflowTools.format_timestamp(run.info.end_time)\n                        if run.info.end_time\n                        else None,\n                        \"metrics\": run_metrics,\n                    }\n                except Exception as e:\n                    logger.warning(\n                        f\"Error getting run details for {version.run_id}: {e!s}\",\n                    )\n                    version_info[\"run\"] = \"Error retrieving run details\"\n\n            model_info[\"versions\"].append(version_info)\n\n        return json.dumps(model_info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting model details: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Get information about the MLflow tracking server and system.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing system information.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_system_info() -&gt; str:\n    \"\"\"Get information about the MLflow tracking server and system.\n\n    Returns:\n        A JSON string containing system information.\n\n    \"\"\"\n    logger.debug(\"Getting MLflow system information\")\n\n    try:\n        info = {\n            \"mlflow_version\": mlflow.__version__,\n            \"tracking_uri\": mlflow.get_tracking_uri(),\n            \"registry_uri\": mlflow.get_registry_uri(),\n            \"artifact_uri\": mlflow.get_artifact_uri(),\n            \"python_version\": sys.version,\n            \"server_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n\n        # Get experiment count\n        try:\n            experiments = client.search_experiments()\n            info[\"experiment_count\"] = len(experiments)\n        except Exception as e:\n            logger.warning(f\"Error getting experiment count: {e!s}\")\n            info[\"experiment_count\"] = \"Error retrieving count\"\n\n        # Get model count\n        try:\n            models = client.search_registered_models()\n            info[\"model_count\"] = len(models)\n        except Exception as e:\n            logger.warning(f\"Error getting model count: {e!s}\")\n            info[\"model_count\"] = \"Error retrieving count\"\n\n        # Get active run count\n        try:\n            active_runs = 0\n            for exp in experiments:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id],\n                    filter_string=\"attributes.status = 'RUNNING'\",\n                    max_results=1000,\n                )\n                active_runs += len(runs)\n\n            info[\"active_runs\"] = active_runs\n        except Exception as e:\n            logger.warning(f\"Error getting active run count: {e!s}\")\n            info[\"active_runs\"] = \"Error retrieving count\"\n\n        return json.dumps(info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting system info: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.list_experiments","title":"<code>list_experiments(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all experiments in the MLflow tracking server, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include experiments whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all experiments matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_experiments(\n    name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS,\n) -&gt; str:\n    \"\"\"List all experiments in the MLflow tracking server, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include experiments whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all experiments matching the criteria.\n\n    \"\"\"\n    logger.debug(f\"Fetching experiments (filter: '{name_contains}', max: {max_results})\")\n\n    try:\n        # Get all experiments\n        experiments = client.search_experiments()\n\n        # Filter by name if specified\n        if name_contains:\n            experiments = [\n                exp for exp in experiments if name_contains.lower() in exp.name.lower()\n            ]\n\n        # Limit to max_results\n        experiments = experiments[:max_results]\n\n        # Create a list to hold experiment information\n        experiments_info = []\n\n        # Extract relevant information for each experiment\n        for exp in experiments:\n            exp_info = {\n                \"experiment_id\": exp.experiment_id,\n                \"name\": exp.name,\n                \"artifact_location\": exp.artifact_location,\n                \"lifecycle_stage\": exp.lifecycle_stage,\n                \"creation_time\": MLflowTools.format_timestamp(exp.creation_time)\n                if hasattr(exp, \"creation_time\")\n                else None,\n                \"tags\": {tag.key: tag.value for tag in exp.tags}\n                if hasattr(exp, \"tags\")\n                else {},\n            }\n\n            # Get the run count for this experiment\n            try:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id], max_results=1,\n                )\n                if runs:\n                    # Just get the count of runs, not the actual runs\n                    run_count = client.search_runs(\n                        experiment_ids=[exp.experiment_id], max_results=1000,\n                    )\n                    exp_info[\"run_count\"] = len(run_count)\n                else:\n                    exp_info[\"run_count\"] = 0\n            except Exception as e:\n                logger.warning(\n                    f\"Error getting run count for experiment {exp.experiment_id}: {e!s}\",\n                )\n                exp_info[\"run_count\"] = \"Error getting count\"\n\n            experiments_info.append(exp_info)\n\n        result = {\n            \"total_experiments\": len(experiments_info),\n            \"experiments\": experiments_info,\n        }\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing experiments: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.tools.list_models","title":"<code>list_models(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all registered models in the MLflow model registry, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include models whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all registered models matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_models(name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS) -&gt; str:\n    \"\"\"List all registered models in the MLflow model registry, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include models whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all registered models matching the criteria.\n\n    \"\"\"\n    logger.debug(\n        f\"Fetching registered models (filter: '{name_contains}', max: {max_results})\",\n    )\n\n    try:\n        # Get all registered models\n        registered_models = client.search_registered_models(max_results=max_results)\n\n        # Filter by name if specified\n        if name_contains:\n            registered_models = [\n                model\n                for model in registered_models\n                if name_contains.lower() in model.name.lower()\n            ]\n\n        # Create a list to hold model information\n        models_info = []\n\n        # Extract relevant information for each model\n        for model in registered_models:\n            model_info = {\n                \"name\": model.name,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    model.creation_timestamp,\n                ),\n                \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                    model.last_updated_timestamp,\n                ),\n                \"description\": model.description or \"\",\n                \"tags\": {tag.key: tag.value for tag in model.tags}\n                if hasattr(model, \"tags\")\n                else {},\n                \"latest_versions\": [],\n            }\n\n            # Add the latest versions if available\n            if model.latest_versions and len(model.latest_versions) &gt; 0:\n                for version in model.latest_versions:\n                    version_info = {\n                        \"version\": version.version,\n                        \"status\": version.status,\n                        \"stage\": version.current_stage,\n                        \"creation_timestamp\": MLflowTools.format_timestamp(\n                            version.creation_timestamp,\n                        ),\n                        \"run_id\": version.run_id,\n                    }\n                    model_info[\"latest_versions\"].append(version_info)\n\n            models_info.append(model_info)\n\n        result = {\"total_models\": len(models_info), \"models\": models_info}\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing models: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.workflow","title":"<code>workflow</code>","text":"<p>Core LangGraph-based workflow engine for processing user queries and generating responses using an AI provider.</p> <p>This workflow supports tool-augmented generation: tool calls are detected and executed in a loop until a final AI response is produced.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.workflow.State","title":"<code>State</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>State schema for the workflow engine.</p>"},{"location":"reference/mlflow_assistant/engine/__init__/#mlflow_assistant.engine.workflow.create_workflow","title":"<code>create_workflow()</code>","text":"<p>Create and return a compiled LangGraph workflow.</p> Source code in <code>src/mlflow_assistant/engine/workflow.py</code> <pre><code>def create_workflow():\n    \"\"\"Create and return a compiled LangGraph workflow.\"\"\"\n    graph_builder = StateGraph(State)\n\n    def call_model(state: State) -&gt; State:\n        \"\"\"Call the AI model and return updated state with response.\"\"\"\n        messages = state[STATE_KEY_MESSAGES]\n        provider_config = state.get(STATE_KEY_PROVIDER_CONFIG, {})\n        try:\n            provider = AIProvider.create(provider_config)\n            model = provider.langchain_model().bind_tools(tools)\n            response = model.invoke(messages)\n            return {**state, STATE_KEY_MESSAGES: [response]}\n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\", exc_info=True)\n            return {**state, STATE_KEY_MESSAGES: messages}\n\n    # Add nodes\n    graph_builder.add_node(\"tools\", ToolNode(tools))\n    graph_builder.add_node(\"model\", call_model)\n\n    # Define graph transitions\n    graph_builder.add_edge(\"tools\", \"model\")\n    graph_builder.add_conditional_edges(\"model\", tools_condition)\n    graph_builder.set_entry_point(\"model\")\n\n    return graph_builder.compile()\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/definitions/","title":"definitions","text":""},{"location":"reference/mlflow_assistant/engine/definitions/#mlflow_assistant.engine.definitions","title":"<code>mlflow_assistant.engine.definitions</code>","text":"<p>Constants for the MLflow Assistant engine.</p>"},{"location":"reference/mlflow_assistant/engine/processor/","title":"processor","text":""},{"location":"reference/mlflow_assistant/engine/processor/#mlflow_assistant.engine.processor","title":"<code>mlflow_assistant.engine.processor</code>","text":"<p>Query processor that leverages the workflow engine for processing user queries and generating responses using an AI provider.</p>"},{"location":"reference/mlflow_assistant/engine/processor/#mlflow_assistant.engine.processor.process_query","title":"<code>process_query(query, provider_config, verbose=False)</code>  <code>async</code>","text":"<p>Process a query through the MLflow Assistant workflow.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to process</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing the response</p> Source code in <code>src/mlflow_assistant/engine/processor.py</code> <pre><code>async def process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Process a query through the MLflow Assistant workflow.\n\n    Args:\n        query: The query to process\n        provider_config: AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dict containing the response\n\n    \"\"\"\n    import time\n\n    from .workflow import create_workflow\n\n    # Track start time for duration calculation\n    start_time = time.time()\n\n    try:\n        # Create workflow\n        workflow = create_workflow()\n\n        # Run workflow with provider config\n        initial_state = {\n            STATE_KEY_MESSAGES: [HumanMessage(content=query)],\n            STATE_KEY_PROVIDER_CONFIG: provider_config,\n        }\n\n        if verbose:\n            logger.info(f\"Running workflow with query: {query}\")\n            logger.info(f\"Using provider: {provider_config.get(CONFIG_KEY_TYPE)}\")\n            logger.info(\n                f\"Using model: {provider_config.get(CONFIG_KEY_MODEL, 'default')}\",\n            )\n\n        result = await workflow.ainvoke(initial_state)\n\n        # Calculate duration\n        duration = time.time() - start_time\n\n        return {\n            \"original_query\": query,\n            \"response\": result.get(STATE_KEY_MESSAGES)[-1],\n            \"duration\": duration,  # Add duration to response\n        }\n\n    except Exception as e:\n        # Calculate duration even for errors\n        duration = time.time() - start_time\n\n        logger.error(f\"Error processing query: {e}\")\n\n        return {\n            \"error\": str(e),\n            \"original_query\": query,\n            \"response\": f\"Error processing query: {e!s}\",\n        }\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/tools/","title":"tools","text":""},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools","title":"<code>mlflow_assistant.engine.tools</code>","text":"<p>LangGraph tools for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.MLflowTools","title":"<code>MLflowTools</code>","text":"<p>Collection of helper utilities for MLflow interactions.</p>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.MLflowTools.format_timestamp","title":"<code>format_timestamp(timestamp_ms)</code>  <code>staticmethod</code>","text":"<p>Convert a millisecond timestamp to a human-readable string.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@staticmethod\ndef format_timestamp(timestamp_ms: int) -&gt; str:\n    \"\"\"Convert a millisecond timestamp to a human-readable string.\"\"\"\n    if not timestamp_ms:\n        return NA\n    dt = datetime.fromtimestamp(timestamp_ms / 1000.0)\n    return dt.strftime(TIME_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.get_model_details","title":"<code>get_model_details(model_name)</code>","text":"<p>Get detailed information about a specific registered model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the registered model</p> required <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing detailed information about the model.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_model_details(model_name: str) -&gt; str:\n    \"\"\"Get detailed information about a specific registered model.\n\n    Args:\n        model_name: The name of the registered model\n\n    Returns:\n        A JSON string containing detailed information about the model.\n\n    \"\"\"\n    logger.debug(f\"Fetching details for model: {model_name}\")\n\n    try:\n        # Get the registered model\n        model = client.get_registered_model(model_name)\n\n        model_info = {\n            \"name\": model.name,\n            \"creation_timestamp\": MLflowTools.format_timestamp(\n                model.creation_timestamp,\n            ),\n            \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                model.last_updated_timestamp,\n            ),\n            \"description\": model.description or \"\",\n            \"tags\": {tag.key: tag.value for tag in model.tags}\n            if hasattr(model, \"tags\")\n            else {},\n            \"versions\": [],\n        }\n\n        # Get all versions for this model\n        versions = client.search_model_versions(f\"name='{model_name}'\")\n\n        for version in versions:\n            version_info = {\n                \"version\": version.version,\n                \"status\": version.status,\n                \"stage\": version.current_stage,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    version.creation_timestamp,\n                ),\n                \"source\": version.source,\n                \"run_id\": version.run_id,\n            }\n\n            # Get additional information about the run if available\n            if version.run_id:\n                try:\n                    run = client.get_run(version.run_id)\n                    # Extract only essential run information to avoid serialization issues\n                    run_metrics = {}\n                    for k, v in run.data.metrics.items():\n                        try:\n                            run_metrics[k] = float(v)\n                        except ValueError:\n                            run_metrics[k] = str(v)\n\n                    version_info[\"run\"] = {\n                        \"status\": run.info.status,\n                        \"start_time\": MLflowTools.format_timestamp(\n                            run.info.start_time,\n                        ),\n                        \"end_time\": MLflowTools.format_timestamp(run.info.end_time)\n                        if run.info.end_time\n                        else None,\n                        \"metrics\": run_metrics,\n                    }\n                except Exception as e:\n                    logger.warning(\n                        f\"Error getting run details for {version.run_id}: {e!s}\",\n                    )\n                    version_info[\"run\"] = \"Error retrieving run details\"\n\n            model_info[\"versions\"].append(version_info)\n\n        return json.dumps(model_info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting model details: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Get information about the MLflow tracking server and system.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing system information.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef get_system_info() -&gt; str:\n    \"\"\"Get information about the MLflow tracking server and system.\n\n    Returns:\n        A JSON string containing system information.\n\n    \"\"\"\n    logger.debug(\"Getting MLflow system information\")\n\n    try:\n        info = {\n            \"mlflow_version\": mlflow.__version__,\n            \"tracking_uri\": mlflow.get_tracking_uri(),\n            \"registry_uri\": mlflow.get_registry_uri(),\n            \"artifact_uri\": mlflow.get_artifact_uri(),\n            \"python_version\": sys.version,\n            \"server_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n\n        # Get experiment count\n        try:\n            experiments = client.search_experiments()\n            info[\"experiment_count\"] = len(experiments)\n        except Exception as e:\n            logger.warning(f\"Error getting experiment count: {e!s}\")\n            info[\"experiment_count\"] = \"Error retrieving count\"\n\n        # Get model count\n        try:\n            models = client.search_registered_models()\n            info[\"model_count\"] = len(models)\n        except Exception as e:\n            logger.warning(f\"Error getting model count: {e!s}\")\n            info[\"model_count\"] = \"Error retrieving count\"\n\n        # Get active run count\n        try:\n            active_runs = 0\n            for exp in experiments:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id],\n                    filter_string=\"attributes.status = 'RUNNING'\",\n                    max_results=1000,\n                )\n                active_runs += len(runs)\n\n            info[\"active_runs\"] = active_runs\n        except Exception as e:\n            logger.warning(f\"Error getting active run count: {e!s}\")\n            info[\"active_runs\"] = \"Error retrieving count\"\n\n        return json.dumps(info, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error getting system info: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.list_experiments","title":"<code>list_experiments(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all experiments in the MLflow tracking server, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include experiments whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all experiments matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_experiments(\n    name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS,\n) -&gt; str:\n    \"\"\"List all experiments in the MLflow tracking server, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include experiments whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all experiments matching the criteria.\n\n    \"\"\"\n    logger.debug(f\"Fetching experiments (filter: '{name_contains}', max: {max_results})\")\n\n    try:\n        # Get all experiments\n        experiments = client.search_experiments()\n\n        # Filter by name if specified\n        if name_contains:\n            experiments = [\n                exp for exp in experiments if name_contains.lower() in exp.name.lower()\n            ]\n\n        # Limit to max_results\n        experiments = experiments[:max_results]\n\n        # Create a list to hold experiment information\n        experiments_info = []\n\n        # Extract relevant information for each experiment\n        for exp in experiments:\n            exp_info = {\n                \"experiment_id\": exp.experiment_id,\n                \"name\": exp.name,\n                \"artifact_location\": exp.artifact_location,\n                \"lifecycle_stage\": exp.lifecycle_stage,\n                \"creation_time\": MLflowTools.format_timestamp(exp.creation_time)\n                if hasattr(exp, \"creation_time\")\n                else None,\n                \"tags\": {tag.key: tag.value for tag in exp.tags}\n                if hasattr(exp, \"tags\")\n                else {},\n            }\n\n            # Get the run count for this experiment\n            try:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id], max_results=1,\n                )\n                if runs:\n                    # Just get the count of runs, not the actual runs\n                    run_count = client.search_runs(\n                        experiment_ids=[exp.experiment_id], max_results=1000,\n                    )\n                    exp_info[\"run_count\"] = len(run_count)\n                else:\n                    exp_info[\"run_count\"] = 0\n            except Exception as e:\n                logger.warning(\n                    f\"Error getting run count for experiment {exp.experiment_id}: {e!s}\",\n                )\n                exp_info[\"run_count\"] = \"Error getting count\"\n\n            experiments_info.append(exp_info)\n\n        result = {\n            \"total_experiments\": len(experiments_info),\n            \"experiments\": experiments_info,\n        }\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing experiments: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/tools/#mlflow_assistant.engine.tools.list_models","title":"<code>list_models(name_contains='', max_results=MLFLOW_MAX_RESULTS)</code>","text":"<p>List all registered models in the MLflow model registry, with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>name_contains</code> <code>str</code> <p>Optional filter to only include models whose names contain this string</p> <code>''</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return (default: 100)</p> <code>MLFLOW_MAX_RESULTS</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing all registered models matching the criteria.</p> Source code in <code>src/mlflow_assistant/engine/tools.py</code> <pre><code>@tool\ndef list_models(name_contains: str = \"\", max_results: int = MLFLOW_MAX_RESULTS) -&gt; str:\n    \"\"\"List all registered models in the MLflow model registry, with optional filtering.\n\n    Args:\n        name_contains: Optional filter to only include models whose names contain this string\n        max_results: Maximum number of results to return (default: 100)\n\n    Returns:\n        A JSON string containing all registered models matching the criteria.\n\n    \"\"\"\n    logger.debug(\n        f\"Fetching registered models (filter: '{name_contains}', max: {max_results})\",\n    )\n\n    try:\n        # Get all registered models\n        registered_models = client.search_registered_models(max_results=max_results)\n\n        # Filter by name if specified\n        if name_contains:\n            registered_models = [\n                model\n                for model in registered_models\n                if name_contains.lower() in model.name.lower()\n            ]\n\n        # Create a list to hold model information\n        models_info = []\n\n        # Extract relevant information for each model\n        for model in registered_models:\n            model_info = {\n                \"name\": model.name,\n                \"creation_timestamp\": MLflowTools.format_timestamp(\n                    model.creation_timestamp,\n                ),\n                \"last_updated_timestamp\": MLflowTools.format_timestamp(\n                    model.last_updated_timestamp,\n                ),\n                \"description\": model.description or \"\",\n                \"tags\": {tag.key: tag.value for tag in model.tags}\n                if hasattr(model, \"tags\")\n                else {},\n                \"latest_versions\": [],\n            }\n\n            # Add the latest versions if available\n            if model.latest_versions and len(model.latest_versions) &gt; 0:\n                for version in model.latest_versions:\n                    version_info = {\n                        \"version\": version.version,\n                        \"status\": version.status,\n                        \"stage\": version.current_stage,\n                        \"creation_timestamp\": MLflowTools.format_timestamp(\n                            version.creation_timestamp,\n                        ),\n                        \"run_id\": version.run_id,\n                    }\n                    model_info[\"latest_versions\"].append(version_info)\n\n            models_info.append(model_info)\n\n        result = {\"total_models\": len(models_info), \"models\": models_info}\n\n        return json.dumps(result, indent=2)\n\n    except Exception as e:\n        error_msg = f\"Error listing models: {e!s}\"\n        logger.error(error_msg, exc_info=True)\n        return json.dumps({\"error\": error_msg})\n</code></pre>"},{"location":"reference/mlflow_assistant/engine/workflow/","title":"workflow","text":""},{"location":"reference/mlflow_assistant/engine/workflow/#mlflow_assistant.engine.workflow","title":"<code>mlflow_assistant.engine.workflow</code>","text":"<p>Core LangGraph-based workflow engine for processing user queries and generating responses using an AI provider.</p> <p>This workflow supports tool-augmented generation: tool calls are detected and executed in a loop until a final AI response is produced.</p>"},{"location":"reference/mlflow_assistant/engine/workflow/#mlflow_assistant.engine.workflow.State","title":"<code>State</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>State schema for the workflow engine.</p>"},{"location":"reference/mlflow_assistant/engine/workflow/#mlflow_assistant.engine.workflow.create_workflow","title":"<code>create_workflow()</code>","text":"<p>Create and return a compiled LangGraph workflow.</p> Source code in <code>src/mlflow_assistant/engine/workflow.py</code> <pre><code>def create_workflow():\n    \"\"\"Create and return a compiled LangGraph workflow.\"\"\"\n    graph_builder = StateGraph(State)\n\n    def call_model(state: State) -&gt; State:\n        \"\"\"Call the AI model and return updated state with response.\"\"\"\n        messages = state[STATE_KEY_MESSAGES]\n        provider_config = state.get(STATE_KEY_PROVIDER_CONFIG, {})\n        try:\n            provider = AIProvider.create(provider_config)\n            model = provider.langchain_model().bind_tools(tools)\n            response = model.invoke(messages)\n            return {**state, STATE_KEY_MESSAGES: [response]}\n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\", exc_info=True)\n            return {**state, STATE_KEY_MESSAGES: messages}\n\n    # Add nodes\n    graph_builder.add_node(\"tools\", ToolNode(tools))\n    graph_builder.add_node(\"model\", call_model)\n\n    # Define graph transitions\n    graph_builder.add_edge(\"tools\", \"model\")\n    graph_builder.add_conditional_edges(\"model\", tools_condition)\n    graph_builder.set_entry_point(\"model\")\n\n    return graph_builder.compile()\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/","title":"providers","text":""},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers","title":"<code>mlflow_assistant.providers</code>","text":"<p>Provider module for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.AIProvider","title":"<code>AIProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.AIProvider.langchain_model","title":"<code>langchain_model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the underlying LangChain model.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.AIProvider.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Auto-register provider subclasses.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Auto-register provider subclasses.\"\"\"\n    super().__init_subclass__(**kwargs)\n    # Register the provider using the class name\n    provider_type = cls.__name__.lower().replace(CONFIG_KEY_PROVIDER, \"\")\n    AIProvider._providers[provider_type] = cls\n    logger.debug(f\"Registered provider: {provider_type}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.AIProvider.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an AI provider based on configuration.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>@classmethod\ndef create(cls, config: dict[str, Any]) -&gt; \"AIProvider\":\n    \"\"\"Create an AI provider based on configuration.\"\"\"\n    provider_type = config.get(CONFIG_KEY_TYPE)\n\n    if not provider_type:\n        error_msg = \"Provider type not specified in configuration\"\n        raise ValueError(error_msg)\n\n    provider_type = provider_type.lower()\n\n    # Extract common parameters\n    kwargs = {}\n    for param in ParameterKeys.PARAMETERS_ALL:\n        if param in config:\n            kwargs[param] = config[param]\n\n    # Import providers dynamically to avoid circular imports\n    if provider_type == Provider.OPENAI.value:\n        from .openai_provider import OpenAIProvider\n\n        logger.debug(\n            f\"Creating OpenAI provider with model {config.get(CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI))}\",\n        )\n        return OpenAIProvider(\n            api_key=config.get(CONFIG_KEY_API_KEY),\n            model=config.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OPENAI),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.OLLAMA.value:\n        from .ollama_provider import OllamaProvider\n\n        logger.debug(\n            f\"Creating Ollama provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return OllamaProvider(\n            uri=config.get(CONFIG_KEY_URI),\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OLLAMA),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.DATABRICKS.value:\n        from .databricks_provider import DatabricksProvider\n\n        logger.debug(\n            f\"Creating Databricks provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return DatabricksProvider(\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.DATABRICKS),\n            ),\n            **kwargs,\n        )\n    if provider_type not in cls._providers:\n        error_msg = f\"Unknown provider type: {provider_type}. Available types: {', '.join(cls._providers.keys())}\"\n        raise ValueError(error_msg)\n    # Generic initialization for future providers\n    provider_class = cls._providers[provider_type]\n    return provider_class(config)\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.get_ollama_models","title":"<code>get_ollama_models(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Fetch the list of available Ollama models.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def get_ollama_models(uri: str = DEFAULT_OLLAMA_URI) -&gt; list:\n    \"\"\"Fetch the list of available Ollama models.\"\"\"\n    # Try using direct API call first\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=10)\n        if response.status_code == 200:\n            data = response.json()\n            models = [m.get(\"name\") for m in data.get(\"models\", [])]\n            if models:\n                return models\n    except Exception as e:\n        logger.debug(f\"Failed to get Ollama models from API: {e}\")\n\n    try:\n        # Execute the Ollama list command\n        ollama_path = shutil.which(\"ollama\")\n\n        result = subprocess.run(  # noqa: S603\n            [ollama_path, \"list\"], capture_output=True, text=True, check=False,\n        )\n\n        # Check if command executed successfully\n        if result.returncode != 0:\n            logger.warning(f\"ollama list failed: {result.stderr}\")\n            return FALLBACK_MODELS\n\n        # Parse the output to extract model names\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) &lt;= 1:  # Only header line or empty\n            return FALLBACK_MODELS\n\n        # Skip header line and extract the first column (model name)\n        models = [line.split()[0] for line in lines[1:]]\n        return models or FALLBACK_MODELS\n\n    except (subprocess.SubprocessError, FileNotFoundError, IndexError) as e:\n        logger.warning(f\"Error fetching Ollama models: {e!s}\")\n        return FALLBACK_MODELS\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.verify_ollama_running","title":"<code>verify_ollama_running(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Verify if Ollama is running at the given URI.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def verify_ollama_running(uri: str = DEFAULT_OLLAMA_URI) -&gt; bool:\n    \"\"\"Verify if Ollama is running at the given URI.\"\"\"\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=2)\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.base","title":"<code>base</code>","text":"<p>Base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.base.AIProvider","title":"<code>AIProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.base.AIProvider.langchain_model","title":"<code>langchain_model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the underlying LangChain model.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.base.AIProvider.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Auto-register provider subclasses.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Auto-register provider subclasses.\"\"\"\n    super().__init_subclass__(**kwargs)\n    # Register the provider using the class name\n    provider_type = cls.__name__.lower().replace(CONFIG_KEY_PROVIDER, \"\")\n    AIProvider._providers[provider_type] = cls\n    logger.debug(f\"Registered provider: {provider_type}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.base.AIProvider.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an AI provider based on configuration.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>@classmethod\ndef create(cls, config: dict[str, Any]) -&gt; \"AIProvider\":\n    \"\"\"Create an AI provider based on configuration.\"\"\"\n    provider_type = config.get(CONFIG_KEY_TYPE)\n\n    if not provider_type:\n        error_msg = \"Provider type not specified in configuration\"\n        raise ValueError(error_msg)\n\n    provider_type = provider_type.lower()\n\n    # Extract common parameters\n    kwargs = {}\n    for param in ParameterKeys.PARAMETERS_ALL:\n        if param in config:\n            kwargs[param] = config[param]\n\n    # Import providers dynamically to avoid circular imports\n    if provider_type == Provider.OPENAI.value:\n        from .openai_provider import OpenAIProvider\n\n        logger.debug(\n            f\"Creating OpenAI provider with model {config.get(CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI))}\",\n        )\n        return OpenAIProvider(\n            api_key=config.get(CONFIG_KEY_API_KEY),\n            model=config.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OPENAI),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.OLLAMA.value:\n        from .ollama_provider import OllamaProvider\n\n        logger.debug(\n            f\"Creating Ollama provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return OllamaProvider(\n            uri=config.get(CONFIG_KEY_URI),\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OLLAMA),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.DATABRICKS.value:\n        from .databricks_provider import DatabricksProvider\n\n        logger.debug(\n            f\"Creating Databricks provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return DatabricksProvider(\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.DATABRICKS),\n            ),\n            **kwargs,\n        )\n    if provider_type not in cls._providers:\n        error_msg = f\"Unknown provider type: {provider_type}. Available types: {', '.join(cls._providers.keys())}\"\n        raise ValueError(error_msg)\n    # Generic initialization for future providers\n    provider_class = cls._providers[provider_type]\n    return provider_class(config)\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.databricks_provider","title":"<code>databricks_provider</code>","text":"<p>Databricks provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.databricks_provider.DatabricksProvider","title":"<code>DatabricksProvider(model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Databricks provider implementation.</p> <p>Initialize the Databricks provider with model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def __init__(\n    self,\n    model: str | None = None,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the Databricks provider with model.\"\"\"\n    self.model_name = (\n        model or Provider.get_default_model(Provider.DATABRICKS.value)\n    )\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.DATABRICKS.value)\n    )\n    self.kwargs = kwargs\n\n    for var in DATABRICKS_CREDENTIALS:\n        if var not in os.environ:\n            logger.warning(\n                f\"Missing environment variable: {var}. \"\n                \"Responses may fail if you are running outside Databricks.\",\n            )\n\n    # Build parameters dict with only non-None values\n    model_params = {\"endpoint\": self.model_name, \"temperature\": temperature}\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.DATABRICKS.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatDatabricks(**model_params)\n\n    logger.debug(f\"Databricks provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.databricks_provider.DatabricksProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.definitions","title":"<code>definitions</code>","text":"<p>Constants for the MLflow Assistant providers.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.definitions.ParameterKeys","title":"<code>ParameterKeys</code>","text":"<p>Keys and default parameter groupings for supported providers.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.definitions.ParameterKeys.get_parameters","title":"<code>get_parameters(provider)</code>  <code>classmethod</code>","text":"<p>Return the list of parameters for the given provider name.</p> Source code in <code>src/mlflow_assistant/providers/definitions.py</code> <pre><code>@classmethod\ndef get_parameters(cls, provider: str) -&gt; list[str]:\n    \"\"\"Return the list of parameters for the given provider name.\"\"\"\n    provider_map = {\n        \"openai\": cls.PARAMETERS_OPENAI,\n        \"ollama\": cls.PARAMETERS_OLLAMA,\n        \"databricks\": cls.PARAMETERS_DATABRICKS,\n    }\n    return provider_map.get(provider.lower(), [])\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.ollama_provider","title":"<code>ollama_provider</code>","text":"<p>Ollama provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.ollama_provider.OllamaProvider","title":"<code>OllamaProvider(uri=None, model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Ollama provider implementation.</p> <p>Initialize the Ollama provider with URI and model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def __init__(self, uri=None, model=None, temperature=None, **kwargs):\n    \"\"\"Initialize the Ollama provider with URI and model.\"\"\"\n    # Handle None URI case to prevent attribute errors\n    if uri is None:\n        logger.warning(\n            f\"Ollama URI is None. Using default URI: {DEFAULT_OLLAMA_URI}\",\n        )\n        self.uri = DEFAULT_OLLAMA_URI\n    else:\n        self.uri = uri.rstrip(\"/\")\n\n    self.model_name = model or OllamaModel.LLAMA32.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OLLAMA.value)\n    )\n\n    # Store kwargs for later use when creating specialized models\n    self.kwargs = kwargs\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"base_url\": self.uri,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Use langchain-ollama's dedicated ChatOllama class\n    self.model = ChatOllama(**model_params)\n\n    logger.debug(\n        f\"Ollama provider initialized with model {self.model_name} at {self.uri}\",\n    )\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.ollama_provider.OllamaProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.openai_provider","title":"<code>openai_provider</code>","text":"<p>OpenAI provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.openai_provider.OpenAIProvider","title":"<code>OpenAIProvider(api_key=None, model=OpenAIModel.GPT35.value, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>OpenAI provider implementation.</p> <p>Initialize the OpenAI provider with API key and model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def __init__(\n    self,\n    api_key=None,\n    model=OpenAIModel.GPT35.value,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the OpenAI provider with API key and model.\"\"\"\n    self.api_key = api_key\n    self.model_name = model or OpenAIModel.GPT35.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OPENAI.value)\n    )\n    self.kwargs = kwargs\n\n    if not self.api_key:\n        logger.warning(\"No OpenAI API key provided. Responses may fail.\")\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"api_key\": api_key,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatOpenAI(**model_params)\n\n    logger.debug(f\"OpenAI provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.openai_provider.OpenAIProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.utilities","title":"<code>utilities</code>","text":"<p>Providers utilities.</p>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.utilities.get_ollama_models","title":"<code>get_ollama_models(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Fetch the list of available Ollama models.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def get_ollama_models(uri: str = DEFAULT_OLLAMA_URI) -&gt; list:\n    \"\"\"Fetch the list of available Ollama models.\"\"\"\n    # Try using direct API call first\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=10)\n        if response.status_code == 200:\n            data = response.json()\n            models = [m.get(\"name\") for m in data.get(\"models\", [])]\n            if models:\n                return models\n    except Exception as e:\n        logger.debug(f\"Failed to get Ollama models from API: {e}\")\n\n    try:\n        # Execute the Ollama list command\n        ollama_path = shutil.which(\"ollama\")\n\n        result = subprocess.run(  # noqa: S603\n            [ollama_path, \"list\"], capture_output=True, text=True, check=False,\n        )\n\n        # Check if command executed successfully\n        if result.returncode != 0:\n            logger.warning(f\"ollama list failed: {result.stderr}\")\n            return FALLBACK_MODELS\n\n        # Parse the output to extract model names\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) &lt;= 1:  # Only header line or empty\n            return FALLBACK_MODELS\n\n        # Skip header line and extract the first column (model name)\n        models = [line.split()[0] for line in lines[1:]]\n        return models or FALLBACK_MODELS\n\n    except (subprocess.SubprocessError, FileNotFoundError, IndexError) as e:\n        logger.warning(f\"Error fetching Ollama models: {e!s}\")\n        return FALLBACK_MODELS\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/__init__/#mlflow_assistant.providers.utilities.verify_ollama_running","title":"<code>verify_ollama_running(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Verify if Ollama is running at the given URI.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def verify_ollama_running(uri: str = DEFAULT_OLLAMA_URI) -&gt; bool:\n    \"\"\"Verify if Ollama is running at the given URI.\"\"\"\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=2)\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/base/","title":"base","text":""},{"location":"reference/mlflow_assistant/providers/base/#mlflow_assistant.providers.base","title":"<code>mlflow_assistant.providers.base</code>","text":"<p>Base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/providers/base/#mlflow_assistant.providers.base.AIProvider","title":"<code>AIProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p>"},{"location":"reference/mlflow_assistant/providers/base/#mlflow_assistant.providers.base.AIProvider.langchain_model","title":"<code>langchain_model</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the underlying LangChain model.</p>"},{"location":"reference/mlflow_assistant/providers/base/#mlflow_assistant.providers.base.AIProvider.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Auto-register provider subclasses.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Auto-register provider subclasses.\"\"\"\n    super().__init_subclass__(**kwargs)\n    # Register the provider using the class name\n    provider_type = cls.__name__.lower().replace(CONFIG_KEY_PROVIDER, \"\")\n    AIProvider._providers[provider_type] = cls\n    logger.debug(f\"Registered provider: {provider_type}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/base/#mlflow_assistant.providers.base.AIProvider.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an AI provider based on configuration.</p> Source code in <code>src/mlflow_assistant/providers/base.py</code> <pre><code>@classmethod\ndef create(cls, config: dict[str, Any]) -&gt; \"AIProvider\":\n    \"\"\"Create an AI provider based on configuration.\"\"\"\n    provider_type = config.get(CONFIG_KEY_TYPE)\n\n    if not provider_type:\n        error_msg = \"Provider type not specified in configuration\"\n        raise ValueError(error_msg)\n\n    provider_type = provider_type.lower()\n\n    # Extract common parameters\n    kwargs = {}\n    for param in ParameterKeys.PARAMETERS_ALL:\n        if param in config:\n            kwargs[param] = config[param]\n\n    # Import providers dynamically to avoid circular imports\n    if provider_type == Provider.OPENAI.value:\n        from .openai_provider import OpenAIProvider\n\n        logger.debug(\n            f\"Creating OpenAI provider with model {config.get(CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI))}\",\n        )\n        return OpenAIProvider(\n            api_key=config.get(CONFIG_KEY_API_KEY),\n            model=config.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OPENAI),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.OLLAMA.value:\n        from .ollama_provider import OllamaProvider\n\n        logger.debug(\n            f\"Creating Ollama provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return OllamaProvider(\n            uri=config.get(CONFIG_KEY_URI),\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.OLLAMA),\n            ),\n            **kwargs,\n        )\n    if provider_type == Provider.DATABRICKS.value:\n        from .databricks_provider import DatabricksProvider\n\n        logger.debug(\n            f\"Creating Databricks provider with model {config.get(CONFIG_KEY_MODEL)}\",\n        )\n        return DatabricksProvider(\n            model=config.get(CONFIG_KEY_MODEL),\n            temperature=config.get(\n                ParameterKeys.TEMPERATURE,\n                Provider.get_default_temperature(Provider.DATABRICKS),\n            ),\n            **kwargs,\n        )\n    if provider_type not in cls._providers:\n        error_msg = f\"Unknown provider type: {provider_type}. Available types: {', '.join(cls._providers.keys())}\"\n        raise ValueError(error_msg)\n    # Generic initialization for future providers\n    provider_class = cls._providers[provider_type]\n    return provider_class(config)\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/databricks_provider/","title":"databricks_provider","text":""},{"location":"reference/mlflow_assistant/providers/databricks_provider/#mlflow_assistant.providers.databricks_provider","title":"<code>mlflow_assistant.providers.databricks_provider</code>","text":"<p>Databricks provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/databricks_provider/#mlflow_assistant.providers.databricks_provider.DatabricksProvider","title":"<code>DatabricksProvider(model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Databricks provider implementation.</p> <p>Initialize the Databricks provider with model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def __init__(\n    self,\n    model: str | None = None,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the Databricks provider with model.\"\"\"\n    self.model_name = (\n        model or Provider.get_default_model(Provider.DATABRICKS.value)\n    )\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.DATABRICKS.value)\n    )\n    self.kwargs = kwargs\n\n    for var in DATABRICKS_CREDENTIALS:\n        if var not in os.environ:\n            logger.warning(\n                f\"Missing environment variable: {var}. \"\n                \"Responses may fail if you are running outside Databricks.\",\n            )\n\n    # Build parameters dict with only non-None values\n    model_params = {\"endpoint\": self.model_name, \"temperature\": temperature}\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.DATABRICKS.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatDatabricks(**model_params)\n\n    logger.debug(f\"Databricks provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/databricks_provider/#mlflow_assistant.providers.databricks_provider.DatabricksProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/databricks_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/definitions/","title":"definitions","text":""},{"location":"reference/mlflow_assistant/providers/definitions/#mlflow_assistant.providers.definitions","title":"<code>mlflow_assistant.providers.definitions</code>","text":"<p>Constants for the MLflow Assistant providers.</p>"},{"location":"reference/mlflow_assistant/providers/definitions/#mlflow_assistant.providers.definitions.ParameterKeys","title":"<code>ParameterKeys</code>","text":"<p>Keys and default parameter groupings for supported providers.</p>"},{"location":"reference/mlflow_assistant/providers/definitions/#mlflow_assistant.providers.definitions.ParameterKeys.get_parameters","title":"<code>get_parameters(provider)</code>  <code>classmethod</code>","text":"<p>Return the list of parameters for the given provider name.</p> Source code in <code>src/mlflow_assistant/providers/definitions.py</code> <pre><code>@classmethod\ndef get_parameters(cls, provider: str) -&gt; list[str]:\n    \"\"\"Return the list of parameters for the given provider name.\"\"\"\n    provider_map = {\n        \"openai\": cls.PARAMETERS_OPENAI,\n        \"ollama\": cls.PARAMETERS_OLLAMA,\n        \"databricks\": cls.PARAMETERS_DATABRICKS,\n    }\n    return provider_map.get(provider.lower(), [])\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/ollama_provider/","title":"ollama_provider","text":""},{"location":"reference/mlflow_assistant/providers/ollama_provider/#mlflow_assistant.providers.ollama_provider","title":"<code>mlflow_assistant.providers.ollama_provider</code>","text":"<p>Ollama provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/ollama_provider/#mlflow_assistant.providers.ollama_provider.OllamaProvider","title":"<code>OllamaProvider(uri=None, model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>Ollama provider implementation.</p> <p>Initialize the Ollama provider with URI and model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def __init__(self, uri=None, model=None, temperature=None, **kwargs):\n    \"\"\"Initialize the Ollama provider with URI and model.\"\"\"\n    # Handle None URI case to prevent attribute errors\n    if uri is None:\n        logger.warning(\n            f\"Ollama URI is None. Using default URI: {DEFAULT_OLLAMA_URI}\",\n        )\n        self.uri = DEFAULT_OLLAMA_URI\n    else:\n        self.uri = uri.rstrip(\"/\")\n\n    self.model_name = model or OllamaModel.LLAMA32.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OLLAMA.value)\n    )\n\n    # Store kwargs for later use when creating specialized models\n    self.kwargs = kwargs\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"base_url\": self.uri,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Use langchain-ollama's dedicated ChatOllama class\n    self.model = ChatOllama(**model_params)\n\n    logger.debug(\n        f\"Ollama provider initialized with model {self.model_name} at {self.uri}\",\n    )\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/ollama_provider/#mlflow_assistant.providers.ollama_provider.OllamaProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/ollama_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/openai_provider/","title":"openai_provider","text":""},{"location":"reference/mlflow_assistant/providers/openai_provider/#mlflow_assistant.providers.openai_provider","title":"<code>mlflow_assistant.providers.openai_provider</code>","text":"<p>OpenAI provider for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/providers/openai_provider/#mlflow_assistant.providers.openai_provider.OpenAIProvider","title":"<code>OpenAIProvider(api_key=None, model=OpenAIModel.GPT35.value, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>AIProvider</code></p> <p>OpenAI provider implementation.</p> <p>Initialize the OpenAI provider with API key and model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def __init__(\n    self,\n    api_key=None,\n    model=OpenAIModel.GPT35.value,\n    temperature: float | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the OpenAI provider with API key and model.\"\"\"\n    self.api_key = api_key\n    self.model_name = model or OpenAIModel.GPT35.value\n    self.temperature = (\n        temperature or Provider.get_default_temperature(Provider.OPENAI.value)\n    )\n    self.kwargs = kwargs\n\n    if not self.api_key:\n        logger.warning(\"No OpenAI API key provided. Responses may fail.\")\n\n    # Build parameters dict with only non-None values\n    model_params = {\n        \"api_key\": api_key,\n        \"model\": self.model_name,\n        \"temperature\": temperature,\n    }\n\n    # Only add optional parameters if they're not None\n    for param in ParameterKeys.get_parameters(Provider.OLLAMA.value):\n        if param in kwargs and kwargs[param] is not None:\n            model_params[param] = kwargs[param]\n\n    # Initialize with parameters matching the documentation\n    self.model = ChatOpenAI(**model_params)\n\n    logger.debug(f\"OpenAI provider initialized with model {self.model_name}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/openai_provider/#mlflow_assistant.providers.openai_provider.OpenAIProvider.langchain_model","title":"<code>langchain_model()</code>","text":"<p>Get the underlying LangChain model.</p> Source code in <code>src/mlflow_assistant/providers/openai_provider.py</code> <pre><code>def langchain_model(self):\n    \"\"\"Get the underlying LangChain model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/utilities/","title":"utilities","text":""},{"location":"reference/mlflow_assistant/providers/utilities/#mlflow_assistant.providers.utilities","title":"<code>mlflow_assistant.providers.utilities</code>","text":"<p>Providers utilities.</p>"},{"location":"reference/mlflow_assistant/providers/utilities/#mlflow_assistant.providers.utilities.get_ollama_models","title":"<code>get_ollama_models(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Fetch the list of available Ollama models.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def get_ollama_models(uri: str = DEFAULT_OLLAMA_URI) -&gt; list:\n    \"\"\"Fetch the list of available Ollama models.\"\"\"\n    # Try using direct API call first\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=10)\n        if response.status_code == 200:\n            data = response.json()\n            models = [m.get(\"name\") for m in data.get(\"models\", [])]\n            if models:\n                return models\n    except Exception as e:\n        logger.debug(f\"Failed to get Ollama models from API: {e}\")\n\n    try:\n        # Execute the Ollama list command\n        ollama_path = shutil.which(\"ollama\")\n\n        result = subprocess.run(  # noqa: S603\n            [ollama_path, \"list\"], capture_output=True, text=True, check=False,\n        )\n\n        # Check if command executed successfully\n        if result.returncode != 0:\n            logger.warning(f\"ollama list failed: {result.stderr}\")\n            return FALLBACK_MODELS\n\n        # Parse the output to extract model names\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) &lt;= 1:  # Only header line or empty\n            return FALLBACK_MODELS\n\n        # Skip header line and extract the first column (model name)\n        models = [line.split()[0] for line in lines[1:]]\n        return models or FALLBACK_MODELS\n\n    except (subprocess.SubprocessError, FileNotFoundError, IndexError) as e:\n        logger.warning(f\"Error fetching Ollama models: {e!s}\")\n        return FALLBACK_MODELS\n</code></pre>"},{"location":"reference/mlflow_assistant/providers/utilities/#mlflow_assistant.providers.utilities.verify_ollama_running","title":"<code>verify_ollama_running(uri=DEFAULT_OLLAMA_URI)</code>","text":"<p>Verify if Ollama is running at the given URI.</p> Source code in <code>src/mlflow_assistant/providers/utilities.py</code> <pre><code>def verify_ollama_running(uri: str = DEFAULT_OLLAMA_URI) -&gt; bool:\n    \"\"\"Verify if Ollama is running at the given URI.\"\"\"\n    try:\n        response = requests.get(f\"{uri}/api/tags\", timeout=2)\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/","title":"utils","text":""},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils","title":"<code>mlflow_assistant.utils</code>","text":"<p>Utility modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n        cls.DATABRICKS: DatabricksModel.DATABRICKS_META_LLAMA3.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Provider.get_default_temperature","title":"<code>get_default_temperature(provider)</code>  <code>classmethod</code>","text":"<p>Get the default temperature for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_temperature(cls, provider):\n    \"\"\"Get the default temperature for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: 0.7,\n        cls.DATABRICKS: 0.7,\n        cls.OLLAMA: 0.7,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    if provider_type == Provider.DATABRICKS.value:\n        # Set environment variables for Databricks profile\n        _set_environment_variables(provider.get(CONFIG_KEY_PROFILE))\n\n        return {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: provider.get(CONFIG_KEY_PROFILE),\n            CONFIG_KEY_MODEL: provider.get(CONFIG_KEY_MODEL),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config","title":"<code>config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    if provider_type == Provider.DATABRICKS.value:\n        # Set environment variables for Databricks profile\n        _set_environment_variables(provider.get(CONFIG_KEY_PROFILE))\n\n        return {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: provider.get(CONFIG_KEY_PROFILE),\n            CONFIG_KEY_MODEL: provider.get(CONFIG_KEY_MODEL),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants","title":"<code>constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.DatabricksModel","title":"<code>DatabricksModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Databricks models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.DatabricksModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Databricks model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Databricks model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n        cls.DATABRICKS: DatabricksModel.DATABRICKS_META_LLAMA3.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Provider.get_default_temperature","title":"<code>get_default_temperature(provider)</code>  <code>classmethod</code>","text":"<p>Get the default temperature for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_temperature(cls, provider):\n    \"\"\"Get the default temperature for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: 0.7,\n        cls.DATABRICKS: 0.7,\n        cls.OLLAMA: 0.7,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions","title":"<code>definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/utils/config/","title":"config","text":""},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config","title":"<code>mlflow_assistant.utils.config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    if provider_type == Provider.DATABRICKS.value:\n        # Set environment variables for Databricks profile\n        _set_environment_variables(provider.get(CONFIG_KEY_PROFILE))\n\n        return {\n            CONFIG_KEY_TYPE: Provider.DATABRICKS.value,\n            CONFIG_KEY_PROFILE: provider.get(CONFIG_KEY_PROFILE),\n            CONFIG_KEY_MODEL: provider.get(CONFIG_KEY_MODEL),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/","title":"constants","text":""},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants","title":"<code>mlflow_assistant.utils.constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.DatabricksModel","title":"<code>DatabricksModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Databricks models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.DatabricksModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Databricks model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Databricks model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n        cls.DATABRICKS: DatabricksModel.DATABRICKS_META_LLAMA3.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Provider.get_default_temperature","title":"<code>get_default_temperature(provider)</code>  <code>classmethod</code>","text":"<p>Get the default temperature for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_temperature(cls, provider):\n    \"\"\"Get the default temperature for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: 0.7,\n        cls.DATABRICKS: 0.7,\n        cls.OLLAMA: 0.7,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/definitions/","title":"definitions","text":""},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions","title":"<code>mlflow_assistant.utils.definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/utils/exceptions/","title":"exceptions","text":""},{"location":"reference/mlflow_assistant/utils/exceptions/#mlflow_assistant.utils.exceptions","title":"<code>mlflow_assistant.utils.exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/exceptions/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"}]}