{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLflow Assistant","text":"<p>A Python package that simplifies working with MLflow by providing streamlined tools and utilities for machine learning experiment tracking and model management.</p>"},{"location":"#what-is-mlflow-assistant","title":"What is MLflow Assistant?","text":"<p>MLflow Assistant enhances your MLflow experience by offering:</p> <ul> <li>Simplified API: Easy-to-use interfaces for common MLflow operations</li> <li>Enhanced Workflows: Streamlined processes for experiment tracking</li> <li>Utility Functions: Helper tools for model management and deployment</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install MLflow Assistant:</p> <pre><code>pip install mlflow-assistant\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation - Setup and installation instructions</li> <li>Code Reference - Complete API documentation</li> <li>Release Notes - What's new in each version</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>MLflow Documentation</li> </ul>"},{"location":"changelog/","title":"CHANGELOG","text":"<p>All notable changes to the <code>mlflow-assistant</code> package will be documented in this file.</p> <p>This project adheres to Semantic Versioning and follows changelog conventions inspired by Keep a Changelog.</p>"},{"location":"changelog/#v014-2025-07-15","title":"[v0.1.4] \u2013 2025-07-15","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Setup This allows users to setup mlflow-assistant.</li> <li>Start This allows users to start the session to interact with mlflow interactively.</li> </ul>"},{"location":"changelog/#notes","title":"Notes","text":"<ul> <li>Start functionality is currently limited as it is calling a mock function</li> </ul>"},{"location":"changelog/#v013-2025-07-15","title":"[v0.1.3] \u2013 2025-07-15","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed navigation structure in <code>mkdocs.yml</code> to correctly point to <code>reference/</code> directory instead of <code>reference/SUMMARY.md</code></li> <li>Improved compatibility with <code>literate-nav</code> and <code>section-index</code> plugins for proper code reference navigation</li> </ul>"},{"location":"changelog/#v012-2025-07-15","title":"[v0.1.2] \u2013 2025-07-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Comprehensive documentation system with MkDocs Material theme</li> <li>Versioned documentation deployment using mike for GitHub Pages</li> <li>Automated documentation generation from code docstrings using mkdocstrings</li> <li>Code reference navigation with auto-generated API documentation</li> <li>Custom Geist font for modern, clean documentation appearance</li> <li>Integrated documentation deployment in the main CI/CD pipeline</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Enhanced CI/CD pipeline to include documentation deployment after successful releases</li> <li>Documentation now automatically deploys on version bumps and PyPI releases</li> </ul>"},{"location":"changelog/#notes_1","title":"Notes","text":"<ul> <li>Documentation is available at https://hugodscarvalho.github.io/mlflow-assistant/</li> <li>Each release gets its own versioned documentation with <code>latest</code> alias pointing to the newest version</li> </ul>"},{"location":"changelog/#v011-2025-07-14","title":"[v0.1.1] \u2013 2025-07-14","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Introduced a unified CI/CD pipeline for:</li> <li>Linting with <code>ruff</code>.</li> <li>Type checking with <code>mypy</code>.</li> <li>Test execution with <code>pytest</code> and coverage reporting via <code>pytest-cov</code>.</li> <li>Publishing to PyPI on new Git tags.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improved test fixture logic for integration tests to correctly check for a live MLflow Tracking Server before execution.</li> </ul>"},{"location":"changelog/#notes_2","title":"Notes","text":"<ul> <li>This pipeline simplifies maintenance by consolidating quality checks and publishing into a single workflow, triggered on pushes and releases.</li> </ul>"},{"location":"changelog/#v010-2025-05-08","title":"[v0.1.0] \u2013 2025-05-08","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Initial placeholder release of <code>mlflow-assistant</code> on PyPI.</li> <li>Included a minimal <code>MLflowClient</code> wrapper module.</li> </ul>"},{"location":"changelog/#notes_3","title":"Notes","text":"<ul> <li>This version was intentionally published with limited functionality to reserve the <code>mlflow-assistant</code> package name on PyPI.</li> <li>Full development will begin in upcoming versions with a focus on making MLflow easier to use through high-level utilities and assistant-like automation features.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides instructions on how to install and get started with <code>mlflow-assistant</code>.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Python: <code>mlflow-assistant</code> requires Python 3.9 or higher. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>or</p> <pre><code>python3 --version\n</code></pre> </li> <li> <p>pip: Python's package installer, <code>pip</code>, is necessary to install <code>mlflow-assistant</code> and its dependencies. It usually comes bundled with Python. You can check if you have it by running:</p> <pre><code>pip --version\n</code></pre> <p>or</p> <pre><code>pip3 --version\n</code></pre> </li> </ul>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":"<p>You can install <code>mlflow-assistant</code> using pip:</p> <pre><code>pip install mlflow-assistant\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mlflow_assistant<ul> <li>cli<ul> <li>commands</li> <li>setup</li> <li>validation</li> </ul> </li> <li>core<ul> <li>cli</li> <li>connection</li> <li>core</li> <li>provider</li> <li>workflow</li> </ul> </li> <li>main</li> <li>utils<ul> <li>config</li> <li>constants</li> <li>definitions</li> <li>exceptions</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlflow_assistant/__init__/","title":"mlflow_assistant","text":""},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant","title":"<code>mlflow_assistant</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli","title":"<code>cli</code>","text":"<p>CLI modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands","title":"<code>commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        _process_user_query(query, provider_config, verbose)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.setup","title":"<code>setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation","title":"<code>validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core","title":"<code>core</code>","text":"<p>Core functionality for MLflow Assistant.</p> <p>This subpackage contains the core modules for managing connections, workflows, and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.cli","title":"<code>cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection","title":"<code>connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.info(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.info(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core","title":"<code>core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.provider","title":"<code>provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.core.workflow","title":"<code>workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.main","title":"<code>main</code>","text":"<p>Main entry point for executing the MLflow Assistant package directly.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils","title":"<code>utils</code>","text":"<p>Utility modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config","title":"<code>config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants","title":"<code>constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions","title":"<code>definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/__init__/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/main/","title":"main","text":""},{"location":"reference/mlflow_assistant/main/#mlflow_assistant.main","title":"<code>mlflow_assistant.main</code>","text":"<p>Main entry point for executing the MLflow Assistant package directly.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/","title":"cli","text":""},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli","title":"<code>mlflow_assistant.cli</code>","text":"<p>CLI modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands","title":"<code>commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        _process_user_query(query, provider_config, verbose)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.setup","title":"<code>setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation","title":"<code>validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/__init__/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/","title":"commands","text":""},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands","title":"<code>mlflow_assistant.cli.commands</code>","text":"<p>CLI commands for MLflow Assistant.</p> <p>This module contains the main CLI commands for interacting with MLflow using natural language queries through various AI providers.</p>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.cli","title":"<code>cli(verbose)</code>","text":"<p>MLflow Assistant: Interact with MLflow using LLMs.</p> <p>This CLI tool helps you to interact with MLflow using natural language.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@click.group()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Enable verbose logging\")\ndef cli(verbose):\n    \"\"\"MLflow Assistant: Interact with MLflow using LLMs.\n\n    This CLI tool helps you to interact with MLflow using natural language.\n    \"\"\"\n    # Configure logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format=LOG_FORMAT)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.mock_process_query","title":"<code>mock_process_query(query, provider_config, verbose=False)</code>","text":"<p>Mock function that simulates the query processing workflow.</p> <p>This will be replaced with the actual implementation later.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's query</p> required <code>provider_config</code> <code>dict[str, Any]</code> <p>The AI provider configuration</p> required <code>verbose</code> <code>bool</code> <p>Whether to show verbose output</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with mock response information</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>def mock_process_query(\n    query: str, provider_config: dict[str, Any], verbose: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"Mock function that simulates the query processing workflow.\n\n    This will be replaced with the actual implementation later.\n\n    Args:\n        query: The user's query\n        provider_config: The AI provider configuration\n        verbose: Whether to show verbose output\n\n    Returns:\n        Dictionary with mock response information\n\n    \"\"\"\n    # Create a mock response\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    response_text = (\n        f\"This is a mock response to: '{query}'\\n\\n\"\n        f\"The MLflow integration will be implemented soon!\"\n    )\n\n    if verbose:\n        response_text += f\"\\n\\nDebug: Using {provider_type} with {model}\"\n\n    return {\n        \"original_query\": query,\n        \"provider_config\": {\n            CONFIG_KEY_TYPE: provider_type,\n            CONFIG_KEY_MODEL: model,\n        },\n        \"enhanced\": False,\n        \"response\": response_text,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.setup","title":"<code>setup()</code>","text":"<p>Run the interactive setup wizard.</p> <p>This wizard helps you configure MLflow Assistant.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef setup():\n    \"\"\"Run the interactive setup wizard.\n\n    This wizard helps you configure MLflow Assistant.\n    \"\"\"\n    setup_wizard()\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.start","title":"<code>start(verbose)</code>","text":"<p>Start an interactive chat session with MLflow Assistant.</p> <p>This opens an interactive chat session where you can ask questions about your MLflow experiments, models, and data. Type /bye to exit the session.</p> <p>Examples of questions you can ask: - What are my best performing models for classification? - Show me details of experiment 'customer_churn' - Compare runs abc123 and def456 - Which hyperparameters should I try next for my regression model?</p> <p>Commands: - /bye: Exit the chat session - /help: Show help about available commands - /clear: Clear the screen</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Show verbose output\")\ndef start(verbose):\n    \"\"\"Start an interactive chat session with MLflow Assistant.\n\n    This opens an interactive chat session where you can ask questions about\n    your MLflow experiments, models, and data. Type /bye to exit the session.\n\n    Examples of questions you can ask:\n    - What are my best performing models for classification?\n    - Show me details of experiment 'customer_churn'\n    - Compare runs abc123 and def456\n    - Which hyperparameters should I try next for my regression model?\n\n    Commands:\n    - /bye: Exit the chat session\n    - /help: Show help about available commands\n    - /clear: Clear the screen\n    \"\"\"\n    # Use validation function to check setup\n    is_valid, error_message = validate_setup()\n    if not is_valid:\n        click.echo(f\"\u274c Error: {error_message}\")\n        return\n\n    # Get provider config\n    provider_config = get_provider_config()\n\n    # Print welcome message and instructions\n    provider_type = provider_config.get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    model = provider_config.get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n\n    click.echo(\"\\n\ud83e\udd16 MLflow Assistant Chat Session\")\n    click.echo(f\"Connected to MLflow at: {get_mlflow_uri()}\")\n    click.echo(f\"Using {provider_type.upper()} with model: {model}\")\n    click.echo(\"\\nType your questions and press Enter.\")\n    click.echo(f\"Type {Command.EXIT.value} to exit.\")\n    click.echo(\"=\" * 70)\n\n    # Start interactive loop\n    while True:\n        # Get user input with a prompt\n        try:\n            query = click.prompt(\"\\n\ud83e\uddd1\", prompt_suffix=\"\").strip()\n        except (KeyboardInterrupt, EOFError):\n            click.echo(\"\\nExiting chat session...\")\n            break\n\n        # Handle special commands\n        action = _handle_special_commands(query)\n        if action == \"exit\":\n            break\n        if action == \"continue\":\n            continue\n\n        # Process the query\n        _process_user_query(query, provider_config, verbose)\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/commands/#mlflow_assistant.cli.commands.version","title":"<code>version()</code>","text":"<p>Show MLflow Assistant version information.</p> Source code in <code>src/mlflow_assistant/cli/commands.py</code> <pre><code>@cli.command()\ndef version():\n    \"\"\"Show MLflow Assistant version information.\"\"\"\n    from mlflow_assistant import __version__\n\n    click.echo(f\"MLflow Assistant version: {__version__}\")\n\n    # Show configuration\n    config = load_config()\n    mlflow_uri = config.get(\n        CONFIG_KEY_MLFLOW_URI, DEFAULT_STATUS_NOT_CONFIGURED,\n        )\n    provider = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_TYPE, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n    model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n        CONFIG_KEY_MODEL, DEFAULT_STATUS_NOT_CONFIGURED,\n    )\n\n    click.echo(f\"MLflow URI: {mlflow_uri}\")\n    click.echo(f\"Provider: {provider}\")\n    click.echo(f\"Model: {model}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/setup/","title":"setup","text":""},{"location":"reference/mlflow_assistant/cli/setup/#mlflow_assistant.cli.setup","title":"<code>mlflow_assistant.cli.setup</code>","text":"<p>Setup wizard for MLflow Assistant configuration.</p> <p>This module provides an interactive setup wizard that guides users through configuring MLflow Assistant, including MLflow connection settings and AI provider configuration (OpenAI or Ollama).</p>"},{"location":"reference/mlflow_assistant/cli/setup/#mlflow_assistant.cli.setup.setup_wizard","title":"<code>setup_wizard()</code>","text":"<p>Interactive setup wizard for mlflow-assistant.</p> Source code in <code>src/mlflow_assistant/cli/setup.py</code> <pre><code>def setup_wizard():\n    \"\"\"Interactive setup wizard for mlflow-assistant.\"\"\"\n    click.echo(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502             MLflow Assistant Setup Wizard            \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n\n    click.echo(\"\\nThis wizard will help you configure MLflow Assistant.\")\n\n    # Initialize config\n    config = load_config()\n    previous_provider = config.get(\n        CONFIG_KEY_PROVIDER, {}).get(CONFIG_KEY_TYPE)\n\n    # MLflow URI\n    mlflow_uri = click.prompt(\n        \"Enter your MLflow URI\",\n        default=config.get(CONFIG_KEY_MLFLOW_URI, DEFAULT_MLFLOW_URI),\n    )\n\n    if not validate_mlflow_uri(mlflow_uri):\n        click.echo(\"\\n\u26a0\ufe0f  Warning: Could not connect to MLflow URI.\")\n        click.echo(\n            \"    Please ensure MLflow is running.\",\n        )\n        click.echo(\n            \"    Common MLflow URLs: http://localhost:5000, \"\n            \"http://localhost:8080\",\n        )\n        if not click.confirm(\n            \"Continue anyway? (Choose Yes if you're sure MLflow is running)\",\n        ):\n            click.echo(\n                \"Setup aborted. \"\n                \"Please ensure MLflow is running and try again.\")\n            return\n        click.echo(\"Continuing with setup using the provided MLflow URI.\")\n    else:\n        click.echo(\"\u2705 Successfully connected to MLflow!\")\n\n    config[CONFIG_KEY_MLFLOW_URI] = mlflow_uri\n\n    # AI Provider\n    provider_options = [p.value.capitalize() for p in Provider]\n    provider_choice = click.prompt(\n        \"\\nWhich AI provider would you like to use?\",\n        type=click.Choice(provider_options, case_sensitive=False),\n        default=config.get(CONFIG_KEY_PROVIDER, {})\n        .get(CONFIG_KEY_TYPE, Provider.OPENAI.value)\n        .capitalize(),\n    )\n\n    current_provider_type = provider_choice.lower()\n    provider_config = {}\n\n    # Check if provider is changing and handle default models\n    provider_changed = (previous_provider and\n                        previous_provider != current_provider_type)\n\n    if current_provider_type == Provider.OPENAI.value:\n        # If switching from another provider, show a message\n        if provider_changed:\n            click.echo(\"\\n\u2705 Switching to OpenAI provider\")\n\n        # Initialize provider config\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OPENAI,\n            ),  # Will be updated after user selection\n        }\n\n        # Check for OpenAI API key\n        api_key = os.environ.get(OPENAI_API_KEY_ENV)\n        if not api_key:\n            click.echo(\n                \"\\n\u26a0\ufe0f  OpenAI API key not found in environment variables.\",\n            )\n            click.echo(\n                f\"Please export your OpenAI API key as {OPENAI_API_KEY_ENV}.\",\n            )\n            click.echo(f\"Example: export {OPENAI_API_KEY_ENV}='your-key-here'\")\n            if not click.confirm(\"Continue without API key?\"):\n                click.echo(\n                    \"Setup aborted. Please set the API key and try again.\",\n                )\n                return\n        else:\n            click.echo(\"\u2705 Found OpenAI API key in environment!\")\n\n        # Always ask for model choice\n        model_choices = OpenAIModel.choices()\n\n        # If changing providers, suggest the default,\n        # otherwise use previous config\n        if provider_changed:\n            suggested_model = Provider.get_default_model(Provider.OPENAI)\n        else:\n            current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            )\n            suggested_model = (\n                current_model\n                if current_model in model_choices\n                else Provider.get_default_model(Provider.OPENAI)\n            )\n\n        model = click.prompt(\n            \"Choose an OpenAI model\",\n            type=click.Choice(model_choices, case_sensitive=False),\n            default=suggested_model,\n        )\n        provider_config[CONFIG_KEY_MODEL] = model\n\n    elif current_provider_type == Provider.OLLAMA.value:\n        # If switching from another provider, automatically set defaults\n        if provider_changed:\n            click.echo(\n                \"\\n\u2705 Switching to Ollama provider with default URI and model\",\n            )\n\n        # Ollama configuration - always ask for URI\n        ollama_uri = click.prompt(\n            \"\\nEnter your Ollama server URI\",\n            default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                CONFIG_KEY_URI, DEFAULT_OLLAMA_URI,\n            ),\n        )\n\n        # Initialize provider config with default model and user-specified URI\n        provider_config = {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: ollama_uri,\n            CONFIG_KEY_MODEL: Provider.get_default_model(\n                Provider.OLLAMA,\n            ),  # Will be updated if user selects a different model\n        }\n\n        # Check if Ollama is running\n        is_connected, ollama_data = validate_ollama_connection(ollama_uri)\n        if is_connected:\n            click.echo(\"\u2705 Ollama server is running!\")\n\n            # Get available models\n            available_models = ollama_data.get(\"models\", [])\n\n            if available_models:\n                click.echo(\n                    f\"\\nAvailable Ollama models: {', '.join(available_models)}\",\n                )\n\n                # If changing providers, suggest the default,\n                # otherwise use previous config\n                default_model = Provider.get_default_model(Provider.OLLAMA)\n                if provider_changed:\n                    suggested_model = (\n                        default_model\n                        if default_model in available_models\n                        else available_models[0]\n                    )\n                else:\n                    current_model = config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL,\n                    )\n                    suggested_model = (\n                        current_model\n                        if current_model in available_models\n                        else default_model\n                    )\n\n                ollama_model = click.prompt(\n                    \"Choose an Ollama model\",\n                    type=click.Choice(available_models, case_sensitive=True),\n                    default=suggested_model,\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n            else:\n                click.echo(\"\\nNo models found. Using default model.\")\n                ollama_model = click.prompt(\n                    \"Enter the Ollama model to use\",\n                    default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                        CONFIG_KEY_MODEL, Provider.get_default_model(\n                            Provider.OLLAMA,\n                        ),\n                    ),\n                )\n                provider_config[CONFIG_KEY_MODEL] = ollama_model\n        else:\n            click.echo(\n                \"\\n\u26a0\ufe0f  Warning: Ollama server not running or\"\n                \" not accessible at this URI.\",\n            )\n            if not click.confirm(\"Continue anyway?\"):\n                click.echo(\n                    \"Setup aborted. Please start Ollama server and try again.\",\n                )\n                return\n\n            # Still prompt for model name\n            ollama_model = click.prompt(\n                \"Enter the Ollama model to use\",\n                default=config.get(CONFIG_KEY_PROVIDER, {}).get(\n                    CONFIG_KEY_MODEL, Provider.get_default_model(\n                        Provider.OLLAMA,\n                    ),\n                ),\n            )\n            provider_config[CONFIG_KEY_MODEL] = ollama_model\n\n    config[CONFIG_KEY_PROVIDER] = provider_config\n\n    # Save the configuration\n    save_config(config)\n\n    click.echo(\"\\n\u2705 Configuration saved successfully!\")\n    click.echo(\"\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n    click.echo(\"\u2502               Getting Started                    \u2502\")\n    click.echo(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n    click.echo(\n        \"\\nYou can now use MLflow Assistant with the following commands:\")\n    click.echo(\n        \"  mlflow-assistant start     - Start an interactive chat \"\n        \"session.\",\n    )\n    click.echo(\n        \"  mlflow-assistant version   - Show version \"\n        \"information.\",\n    )\n\n    click.echo(\"\\nFor more information, use 'mlflow-assistant --help'\")\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/","title":"validation","text":""},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation","title":"<code>mlflow_assistant.cli.validation</code>","text":"<p>Validation utilities for MLflow Assistant configuration.</p> <p>This module provides validation functions to check MLflow connections, AI provider configurations, and overall system setup to ensure proper operation of MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_mlflow_uri","title":"<code>validate_mlflow_uri(uri)</code>","text":"<p>Validate MLflow URI by attempting to connect.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MLflow server URI</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_mlflow_uri(uri: str) -&gt; bool:\n    \"\"\"Validate MLflow URI by attempting to connect.\n\n    Args:\n        uri: MLflow server URI\n\n    Returns:\n        bool: True if connection successful, False otherwise\n\n    \"\"\"\n    for endpoint in MLFLOW_VALIDATION_ENDPOINTS:\n        try:\n            # Try with trailing slash trimmed\n            clean_uri = uri.rstrip(\"/\")\n            url = f\"{clean_uri}{endpoint}\"\n            logger.debug(f\"Trying to connect to MLflow at: {url}\")\n\n            response = requests.get(url, timeout=MLFLOW_CONNECTION_TIMEOUT)\n            if response.status_code == 200:\n                logger.info(f\"Successfully connected to MLflow at {url}\")\n                return True\n            logger.debug(f\"Response from {url}: {response.status_code}\")\n        except Exception as e:\n            logger.debug(f\"Failed to connect to {endpoint}: {e!s}\")\n\n    # If we get here, none of the endpoints worked\n    logger.warning(\n        f\"Could not validate MLflow at {uri} on any standard endpoint\",\n    )\n    return False\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_ollama_connection","title":"<code>validate_ollama_connection(uri)</code>","text":"<p>Validate Ollama connection and get available models.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Ollama server URI</p> required <p>Returns:</p> Type Description <code>tuple[bool, dict[str, Any]]</code> <p>Tuple[bool, Dict[str, Any]]: (is_valid, response_data)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_ollama_connection(uri: str) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Validate Ollama connection and get available models.\n\n    Args:\n        uri: Ollama server URI\n\n    Returns:\n        Tuple[bool, Dict[str, Any]]: (is_valid, response_data)\n\n    \"\"\"\n    try:\n        response = requests.get(\n            f\"{uri}{OLLAMA_TAGS_ENDPOINT}\", timeout=OLLAMA_CONNECTION_TIMEOUT,\n        )\n        if response.status_code == 200:\n            try:\n                models_data = response.json()\n                available_models = [\n                    m.get(\"name\") for m in models_data.get(\"models\", [])\n                ]\n                return True, {\"models\": available_models}\n            except Exception as e:\n                logger.debug(f\"Error parsing Ollama models: {e}\")\n                return True, {\"models\": []}\n        else:\n            return False, {}\n    except Exception as e:\n        logger.debug(f\"Error connecting to Ollama: {e}\")\n        return False, {}\n</code></pre>"},{"location":"reference/mlflow_assistant/cli/validation/#mlflow_assistant.cli.validation.validate_setup","title":"<code>validate_setup(check_api_key=True)</code>","text":"<p>Validate that MLflow Assistant is properly configured.</p> <p>Parameters:</p> Name Type Description Default <code>check_api_key</code> <code>bool</code> <p>Whether to check for API key if using OpenAI</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple[bool, str]: (is_valid, error_message)</p> Source code in <code>src/mlflow_assistant/cli/validation.py</code> <pre><code>def validate_setup(check_api_key: bool = True) -&gt; tuple[bool, str]:\n    \"\"\"Validate that MLflow Assistant is properly configured.\n\n    Args:\n        check_api_key: Whether to check for API key if using OpenAI\n\n    Returns:\n        Tuple[bool, str]: (is_valid, error_message)\n\n    \"\"\"\n    # Check MLflow URI\n    mlflow_uri = get_mlflow_uri()\n    if not mlflow_uri:\n        return (\n            False,\n            \"MLflow URI not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Get provider config\n    provider_config = get_provider_config()\n    if not provider_config or not provider_config.get(CONFIG_KEY_TYPE):\n        return (\n            False,\n            \"AI provider not configured. \"\n            \"Run 'mlflow-assistant setup' first.\",\n        )\n\n    # Ensure OpenAI has an API key if that's the configured provider\n    if (\n        check_api_key\n        and provider_config.get(CONFIG_KEY_TYPE) == Provider.OPENAI.value\n        and not provider_config.get(CONFIG_KEY_API_KEY)\n    ):\n        return (\n            False,\n            f\"OpenAI API key not found in environment. \"\n            f\"Set {OPENAI_API_KEY_ENV}.\",\n        )\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/","title":"core","text":""},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core","title":"<code>mlflow_assistant.core</code>","text":"<p>Core functionality for MLflow Assistant.</p> <p>This subpackage contains the core modules for managing connections, workflows, and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.cli","title":"<code>cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection","title":"<code>connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.info(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.info(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core","title":"<code>core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.provider","title":"<code>provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/core/__init__/#mlflow_assistant.core.workflow","title":"<code>workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/core/cli/","title":"cli","text":""},{"location":"reference/mlflow_assistant/core/cli/#mlflow_assistant.core.cli","title":"<code>mlflow_assistant.core.cli</code>","text":"<p>Command-line interface (CLI) for MLflow Assistant.</p> <p>This module provides the CLI entry points for interacting with the MLflow Assistant, allowing users to manage connections, workflows, and other operations via the command line.</p>"},{"location":"reference/mlflow_assistant/core/connection/","title":"connection","text":""},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection","title":"<code>mlflow_assistant.core.connection</code>","text":"<p>MLflow connection module for handling connections to MLflow Tracking Server.</p> <p>This module provides functionality to connect to both local and remote MLflow Tracking Servers using environment variables or direct configuration.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection","title":"<code>MLflowConnection(tracking_uri=None, client_factory=None)</code>","text":"<p>MLflow connection class to handle connections to MLflow Tracking Server.</p> <p>This class provides functionality to connect to both local and remote MLflow Tracking Servers.</p> <p>Initialize MLflow connection.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str | None</code> <p>URI of the MLflow Tracking Server. If None, will try to get from environment.</p> <code>None</code> <code>client_factory</code> <code>Any</code> <p>A callable to create the MlflowClient instance. Defaults to MlflowClient.</p> <code>None</code> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def __init__(self, tracking_uri: str | None = None, client_factory: Any = None):\n    \"\"\"Initialize MLflow connection.\n\n    Args:\n        tracking_uri: URI of the MLflow Tracking Server. If None, will try to get from environment.\n        client_factory: A callable to create the MlflowClient instance. Defaults to MlflowClient.\n\n    \"\"\"\n    self.config = self._load_config(tracking_uri=tracking_uri)\n    self.client = None\n    self.is_connected_flag = False\n    self.client_factory = client_factory or MlflowClient\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.connect","title":"<code>connect()</code>","text":"<p>Connect to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.connect--returns","title":"Returns","text":"<pre><code>Tuple[bool, str]: (success, message)\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def connect(self) -&gt; tuple[bool, str]:\n    \"\"\"Connect to MLflow Tracking Server.\n\n    Returns\n    -------\n        Tuple[bool, str]: (success, message)\n\n    \"\"\"\n    try:\n        logger.info(f\"Connecting to MLflow Tracking Server at {self.config.tracking_uri}\")\n        mlflow.set_tracking_uri(self.config.tracking_uri)\n        self.client = self.client_factory(tracking_uri=self.config.tracking_uri)\n        self.client.search_experiments()  # Trigger connection attempt\n        self.is_connected_flag = True\n        logger.info(f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\")\n        return True, f\"Successfully connected to MLflow Tracking Server at {self.config.tracking_uri}\"\n    except Exception as e:\n        self.is_connected_flag = False\n        logger.exception(f\"Failed to connect to MLflow Tracking Server: {e}\")\n        return False, f\"Failed to connect to MLflow Tracking Server: {e!s}\"\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client","title":"<code>get_client()</code>","text":"<p>Get MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client--returns","title":"Returns","text":"<pre><code>MlflowClient: MLflow client instance.\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_client--raises","title":"Raises","text":"<pre><code>MLflowConnectionError: If not connected to MLflow Tracking Server.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_client(self) -&gt; MlflowClient:\n    \"\"\"Get MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: MLflow client instance.\n\n    Raises\n    ------\n        MLflowConnectionError: If not connected to MLflow Tracking Server.\n\n    \"\"\"\n    if self.client is None:\n        msg = \"Not connected to MLflow Tracking Server. Call connect() first.\"\n        raise MLflowConnectionError(msg)\n    return self.client\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info","title":"<code>get_connection_info()</code>","text":"<p>Get connection information.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.get_connection_info--returns","title":"Returns","text":"<pre><code>Dict[str, Any]: Connection information.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def get_connection_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get connection information.\n\n    Returns\n    -------\n        Dict[str, Any]: Connection information.\n\n    \"\"\"\n    return {\n        \"tracking_uri\": self.config.tracking_uri,\n        \"connection_type\": self.config.connection_type,\n        \"is_connected\": self.is_connected_flag,\n    }\n</code></pre>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if connected to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/connection/#mlflow_assistant.core.connection.MLflowConnection.is_connected--returns","title":"Returns","text":"<pre><code>bool: True if connected, False otherwise.\n</code></pre> Source code in <code>src/mlflow_assistant/core/connection.py</code> <pre><code>def is_connected(self) -&gt; bool:\n    \"\"\"Check if connected to MLflow Tracking Server.\n\n    Returns\n    -------\n        bool: True if connected, False otherwise.\n\n    \"\"\"\n    return self.is_connected_flag\n</code></pre>"},{"location":"reference/mlflow_assistant/core/core/","title":"core","text":""},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core","title":"<code>mlflow_assistant.core.core</code>","text":"<p>Core utilities and functionality for MLflow Assistant.</p> <p>This module provides foundational classes, functions, and utilities used across the MLflow Assistant project, including shared logic for managing workflows and interactions with the MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core.get_mlflow_client","title":"<code>get_mlflow_client()</code>","text":"<p>Initialize and return an MLflow client instance.</p>"},{"location":"reference/mlflow_assistant/core/core/#mlflow_assistant.core.core.get_mlflow_client--returns","title":"Returns","text":"<pre><code>MlflowClient: An instance of the MLflow client.\n</code></pre> Source code in <code>src/mlflow_assistant/core/core.py</code> <pre><code>def get_mlflow_client():\n    \"\"\"Initialize and return an MLflow client instance.\n\n    Returns\n    -------\n        MlflowClient: An instance of the MLflow client.\n\n    \"\"\"\n    return MlflowClient()\n</code></pre>"},{"location":"reference/mlflow_assistant/core/provider/","title":"provider","text":""},{"location":"reference/mlflow_assistant/core/provider/#mlflow_assistant.core.provider","title":"<code>mlflow_assistant.core.provider</code>","text":"<p>Provider integrations for MLflow Assistant.</p> <p>This module defines the interfaces and implementations for integrating with various large language model (LLM) providers, such as OpenAI and Ollama.</p>"},{"location":"reference/mlflow_assistant/core/workflow/","title":"workflow","text":""},{"location":"reference/mlflow_assistant/core/workflow/#mlflow_assistant.core.workflow","title":"<code>mlflow_assistant.core.workflow</code>","text":"<p>Workflow management for LangGraph in MLflow Assistant.</p> <p>This module provides functionality for defining, managing, and executing workflows using LangGraph, enabling seamless integration with MLflow for tracking and managing machine learning workflows.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/","title":"utils","text":""},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils","title":"<code>mlflow_assistant.utils</code>","text":"<p>Utility modules for MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config","title":"<code>config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants","title":"<code>constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions","title":"<code>definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/__init__/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"},{"location":"reference/mlflow_assistant/utils/config/","title":"config","text":""},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config","title":"<code>mlflow_assistant.utils.config</code>","text":"<p>Configuration management utilities for MLflow Assistant.</p> <p>This module provides functions for loading, saving, and accessing configuration settings for MLflow Assistant, including MLflow URI and AI provider settings. Configuration is stored in YAML format in the user's home directory.</p>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.ensure_config_dir","title":"<code>ensure_config_dir()</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def ensure_config_dir():\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    if not CONFIG_DIR.exists():\n        CONFIG_DIR.mkdir(parents=True)\n        logger.info(f\"Created configuration directory at {CONFIG_DIR}\")\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.get_mlflow_uri","title":"<code>get_mlflow_uri()</code>","text":"<p>Get the MLflow URI from config or environment.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Optional[str]: The MLflow URI or None if not configured</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_mlflow_uri() -&gt; str | None:\n    \"\"\"Get the MLflow URI from config or environment.\n\n    Returns:\n        Optional[str]: The MLflow URI or None if not configured\n\n    \"\"\"\n    # Environment variable should take precedence\n    if mlflow_uri_env := os.environ.get(MLFLOW_URI_ENV):\n        return mlflow_uri_env\n\n    # Fall back to config\n    config = load_config()\n    return config.get(CONFIG_KEY_MLFLOW_URI)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.get_provider_config","title":"<code>get_provider_config()</code>","text":"<p>Get the AI provider configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: The provider configuration</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def get_provider_config() -&gt; dict[str, Any]:\n    \"\"\"Get the AI provider configuration.\n\n    Returns:\n        Dict[str, Any]: The provider configuration\n\n    \"\"\"\n    config = load_config()\n    provider = config.get(CONFIG_KEY_PROVIDER, {})\n\n    provider_type = provider.get(CONFIG_KEY_TYPE)\n\n    if provider_type == Provider.OPENAI.value:\n        # Environment variable should take precedence\n        api_key = (os.environ.get(OPENAI_API_KEY_ENV) or\n                   provider.get(CONFIG_KEY_API_KEY))\n        return {\n            CONFIG_KEY_TYPE: Provider.OPENAI.value,\n            CONFIG_KEY_API_KEY: api_key,\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OPENAI),\n            ),\n        }\n\n    if provider_type == Provider.OLLAMA.value:\n        return {\n            CONFIG_KEY_TYPE: Provider.OLLAMA.value,\n            CONFIG_KEY_URI: provider.get(CONFIG_KEY_URI, DEFAULT_OLLAMA_URI),\n            CONFIG_KEY_MODEL: provider.get(\n                CONFIG_KEY_MODEL, Provider.get_default_model(Provider.OLLAMA),\n            ),\n        }\n\n    return {CONFIG_KEY_TYPE: None}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from file.</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def load_config() -&gt; dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    if not CONFIG_FILE.exists():\n        logger.info(f\"No configuration file found at {CONFIG_FILE}\")\n        return {}\n\n    try:\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f) or {}\n            logger.debug(f\"Loaded configuration: {config}\")\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/config/#mlflow_assistant.utils.config.save_config","title":"<code>save_config(config)</code>","text":"<p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to save</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>src/mlflow_assistant/utils/config.py</code> <pre><code>def save_config(config: dict[str, Any]) -&gt; bool:\n    \"\"\"Save configuration to file.\n\n    Args:\n        config: Configuration dictionary to save\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    \"\"\"\n    ensure_config_dir()\n\n    try:\n        with open(CONFIG_FILE, \"w\") as f:\n            yaml.dump(config, f)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving configuration: {e}\")\n        return False\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/","title":"constants","text":""},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants","title":"<code>mlflow_assistant.utils.constants</code>","text":"<p>Constants and enumerations for MLflow Assistant.</p> <p>This module defines configuration keys, default values, API endpoints, model definitions, and other constants used throughout MLflow Assistant. It includes enums for AI providers (OpenAI, Ollama) and their supported models.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special commands for interactive chat sessions.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Command.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the description for a command.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Default Ollama models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OllamaModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available Ollama model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available Ollama model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>OpenAI models supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.OpenAIModel.choices","title":"<code>choices()</code>  <code>classmethod</code>","text":"<p>Get all available OpenAI model choices.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef choices(cls):\n    \"\"\"Get all available OpenAI model choices.\"\"\"\n    return [model.value for model in cls]\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>AI provider types supported by MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/constants/#mlflow_assistant.utils.constants.Provider.get_default_model","title":"<code>get_default_model(provider)</code>  <code>classmethod</code>","text":"<p>Get the default model for a provider.</p> Source code in <code>src/mlflow_assistant/utils/constants.py</code> <pre><code>@classmethod\ndef get_default_model(cls, provider):\n    \"\"\"Get the default model for a provider.\"\"\"\n    defaults = {\n        cls.OPENAI: OpenAIModel.GPT35.value,\n        cls.OLLAMA: OllamaModel.LLAMA32.value,\n    }\n    return defaults.get(provider)\n</code></pre>"},{"location":"reference/mlflow_assistant/utils/definitions/","title":"definitions","text":""},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions","title":"<code>mlflow_assistant.utils.definitions</code>","text":"<p>Constants and definitions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions.MLflowConnectionConfig","title":"<code>MLflowConnectionConfig(tracking_uri)</code>  <code>dataclass</code>","text":"<p>Configuration for MLflow connection.</p>"},{"location":"reference/mlflow_assistant/utils/definitions/#mlflow_assistant.utils.definitions.MLflowConnectionConfig.connection_type","title":"<code>connection_type</code>  <code>property</code>","text":"<p>Return the connection type (local or remote).</p>"},{"location":"reference/mlflow_assistant/utils/exceptions/","title":"exceptions","text":""},{"location":"reference/mlflow_assistant/utils/exceptions/#mlflow_assistant.utils.exceptions","title":"<code>mlflow_assistant.utils.exceptions</code>","text":"<p>Custom exceptions for the MLflow Assistant.</p>"},{"location":"reference/mlflow_assistant/utils/exceptions/#mlflow_assistant.utils.exceptions.MLflowConnectionError","title":"<code>MLflowConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an issue connecting to MLflow Tracking Server.</p>"}]}